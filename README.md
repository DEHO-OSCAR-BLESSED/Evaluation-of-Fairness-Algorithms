# Evaluation-of-Fairness-Algorithms

With the widespread use of Learning Analytics (LA) by educational institutions to drive student success, ethical concerns about fairness, transparency and privacy have been raised. It has been found that LA models may be biased against students of certain demographic groups. Although fairness in Machine Learning (ML) has gained a lot of attention in the last decade with a significant number of works in domains such as criminal justice and employment, it is only recently that attention has been paid to fairness in LA. Despite the considerable number of works on fairness, we are yet to see these fairness algorithms deployed in the relevant real-world domains. The decision on which fairness algorithm or metric to use in a particular context is unknown. On this premise, we performed a comparative evaluation of some selected bias mitigation algorithms regarded in the fair ML community to have shown promising results. We comparatively evaluated how the fairness algorithms contribute to ethical and trustworthy LA by testing for some hypotheses using a wide collection of performance and fairness metrics. The results of our comparative evaluation may to some extent, guide fairness algorithm and metric selection for a given context. Interestingly, our results show how data bias does not always necessarily result in predictive bias. Perhaps not surprisingly, our test for fairness-utility tradeoff shows how ensuring fairness does not always lead to drop in utility. We found that sometimes, ensuring fairness also enhances utility. We recommend how works on fairness in LA should go beyond \textit{equality} of some fairness metric to \textit{equity}
