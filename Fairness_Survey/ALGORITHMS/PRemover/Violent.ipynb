{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Violent.ipynb","provenance":[{"file_id":"1RyrIFd1EbO7wTWXg1693zq8SahcbsU8L","timestamp":1628685738191},{"file_id":"1zUVqlZwnp2gUWV_U6552PNihQarJC9I9","timestamp":1628683226587},{"file_id":"1AJWD--4mn4SFR1x2nX38Hkscv0C2Ovqi","timestamp":1628499839227}],"authorship_tag":"ABX9TyO4+RfYq5hPW2OTLSdxNdWT"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ff_Jv8Ptu4lP"},"source":["\n","# INSTALLATION"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q5qYRG8zufHw","executionInfo":{"status":"ok","timestamp":1629679406632,"user_tz":-570,"elapsed":15079,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"95901bdf-dc8b-4df2-d3ab-74e10a282b6e"},"source":["!pip install aif360\n","!pip install fairlearn"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting aif360\n","  Downloading aif360-0.4.0-py3-none-any.whl (175 kB)\n","\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 20.2 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 14.1 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 92 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 175 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: scipy<1.6.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.4.1)\n","Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from aif360) (0.22.2.post1)\n","Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.19.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from aif360) (3.2.2)\n","Collecting tempeh\n","  Downloading tempeh-0.1.12-py3-none-any.whl (39 kB)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.1.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->aif360) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->aif360) (1.0.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (2.4.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360) (2.23.0)\n","Collecting shap\n","  Downloading shap-0.39.0.tar.gz (356 kB)\n","\u001b[K     |████████████████████████████████| 356 kB 45.2 MB/s \n","\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360) (3.6.4)\n","Collecting memory-profiler\n","  Downloading memory_profiler-0.58.0.tar.gz (36 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler->tempeh->aif360) (5.4.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (57.4.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (8.8.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (0.7.1)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (21.2.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (1.4.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (1.10.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (2.10)\n","Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360) (4.62.0)\n","Collecting slicer==0.0.7\n","  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n","Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360) (0.51.2)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360) (1.3.0)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap->tempeh->aif360) (0.34.0)\n","Building wheels for collected packages: memory-profiler, shap\n","  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for memory-profiler: filename=memory_profiler-0.58.0-py3-none-any.whl size=30190 sha256=5c542580446a59e76dbbf08d15c84410da9764e24f5c19f7246871bb33d06b85\n","  Stored in directory: /root/.cache/pip/wheels/56/19/d5/8cad06661aec65a04a0d6785b1a5ad035cb645b1772a4a0882\n","  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for shap: filename=shap-0.39.0-cp37-cp37m-linux_x86_64.whl size=491650 sha256=42b9adc5b4af5f2d0678481f1f39dad8a142f093b34286971cb4294c27bfccb9\n","  Stored in directory: /root/.cache/pip/wheels/ca/25/8f/6ae5df62c32651cd719e972e738a8aaa4a87414c4d2b14c9c0\n","Successfully built memory-profiler shap\n","Installing collected packages: slicer, shap, memory-profiler, tempeh, aif360\n","Successfully installed aif360-0.4.0 memory-profiler-0.58.0 shap-0.39.0 slicer-0.0.7 tempeh-0.1.12\n","Collecting fairlearn\n","  Downloading fairlearn-0.7.0-py3-none-any.whl (177 kB)\n","\u001b[K     |████████████████████████████████| 177 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (1.19.5)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (1.4.1)\n","Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (0.22.2.post1)\n","Requirement already satisfied: pandas>=0.25.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (1.1.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.1->fairlearn) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.1->fairlearn) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25.1->fairlearn) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->fairlearn) (1.0.1)\n","Installing collected packages: fairlearn\n","Successfully installed fairlearn-0.7.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TltW3iPkux0Q","executionInfo":{"status":"ok","timestamp":1629679407961,"user_tz":-570,"elapsed":1342,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"d224336e-ae28-4ce3-a32e-a0e31453dc65"},"source":["!apt-get install -jre\n","!java -version"],"execution_count":2,"outputs":[{"output_type":"stream","text":["E: Command line option 'j' [from -jre] is not understood in combination with the other options.\n","openjdk version \"11.0.11\" 2021-04-20\n","OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04)\n","OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KssrNl8GvDYU","executionInfo":{"status":"ok","timestamp":1629679439025,"user_tz":-570,"elapsed":31068,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"e34ec6eb-9857-4806-855d-f66651d39631"},"source":["!pip install h2o"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting h2o\n","  Downloading h2o-3.32.1.6.tar.gz (168.4 MB)\n","\u001b[K     |████████████████████████████████| 168.4 MB 41 kB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from h2o) (2.23.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from h2o) (0.8.9)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from h2o) (0.16.0)\n","Collecting colorama>=0.3.8\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (3.0.4)\n","Building wheels for collected packages: h2o\n","  Building wheel for h2o (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for h2o: filename=h2o-3.32.1.6-py2.py3-none-any.whl size=168439194 sha256=bd0bc9a9ab975bbb5968cd54723bfa7c44a74c556b73de80a70e2574f47c09d9\n","  Stored in directory: /root/.cache/pip/wheels/ee/0f/51/849ba221c4c1b11a04efb4a3427dc9cb1c4dcde218c6c98b13\n","Successfully built h2o\n","Installing collected packages: colorama, h2o\n","Successfully installed colorama-0.4.4 h2o-3.32.1.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_NQn2JJ0uw6u","executionInfo":{"status":"ok","timestamp":1629679442362,"user_tz":-570,"elapsed":3344,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"b301de88-8c6a-4ecf-d0c7-7232514f74fc"},"source":["!pip install xlsxwriter"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting xlsxwriter\n","  Downloading XlsxWriter-3.0.1-py3-none-any.whl (148 kB)\n","\u001b[K     |████████████████████████████████| 148 kB 5.1 MB/s \n","\u001b[?25hInstalling collected packages: xlsxwriter\n","Successfully installed xlsxwriter-3.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0YklbHpAxd8","executionInfo":{"status":"ok","timestamp":1629679448016,"user_tz":-570,"elapsed":5668,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"c8d3fc5f-e77c-4d29-aaef-184d5ff6d0c0"},"source":["!pip install BlackBoxAuditing"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting BlackBoxAuditing\n","  Downloading BlackBoxAuditing-0.1.54.tar.gz (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (2.6.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (3.2.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (1.1.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (1.19.5)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (2.8.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->BlackBoxAuditing) (1.15.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->BlackBoxAuditing) (2018.9)\n","Building wheels for collected packages: BlackBoxAuditing\n","  Building wheel for BlackBoxAuditing (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for BlackBoxAuditing: filename=BlackBoxAuditing-0.1.54-py2.py3-none-any.whl size=1394771 sha256=17319131630158452ddabab53242d9a613c7da47701177281255c5a94392bf21\n","  Stored in directory: /root/.cache/pip/wheels/05/9f/ee/541a74be4cf5dad17430e64d3276370ea7b6a834a76cb4215a\n","Successfully built BlackBoxAuditing\n","Installing collected packages: BlackBoxAuditing\n","Successfully installed BlackBoxAuditing-0.1.54\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-Y_uQ6vdvN4a"},"source":["#IMPORTS"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rf1aISz6vGfR","executionInfo":{"status":"ok","timestamp":1629679458811,"user_tz":-570,"elapsed":10801,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"437b9370-4cd0-47bc-f292-0855180b93cb"},"source":["import numpy as np\n","from mlxtend.feature_selection import  ExhaustiveFeatureSelector\n","from xgboost import  XGBClassifier\n","# import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import openpyxl\n","import xlsxwriter\n","from openpyxl import  load_workbook\n","\n","import BlackBoxAuditing\n","import shap\n","#suppress setwith copy warning\n","pd.set_option('mode.chained_assignment',None)\n","from sklearn.feature_selection import VarianceThreshold\n","from sklearn.feature_selection import SelectKBest, SelectFwe, SelectPercentile,SelectFdr, SelectFpr, SelectFromModel\n","from sklearn.feature_selection import chi2, mutual_info_classif\n","# from skfeature.function.similarity_based import fisher_score\n","from aif360.algorithms.inprocessing import PrejudiceRemover\n","import matplotlib.pyplot as plt\n","from aif360.metrics.classification_metric import ClassificationMetric\n","\n","from aif360.metrics import BinaryLabelDatasetMetric\n","from aif360.algorithms.preprocessing import DisparateImpactRemover, Reweighing, LFR,OptimPreproc\n","from aif360.datasets import StandardDataset , BinaryLabelDataset\n","from sklearn.preprocessing import MinMaxScaler \n","MM= MinMaxScaler()\n","import h2o\n","from h2o.automl import H2OAutoML\n","from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n","\n","import sys\n","sys.path.append(\"../\")\n","import os\n","\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"id":"RcxQeeX7vUXz","executionInfo":{"status":"ok","timestamp":1629682644854,"user_tz":-570,"elapsed":511,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"fe37e289-a25c-4bc5-cdd5-2a74a4bf0ed7"},"source":["h2o.init()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Checking whether there is an H2O instance running at http://localhost:54321 . connected.\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n","<td>53 mins 02 secs</td></tr>\n","<tr><td>H2O_cluster_timezone:</td>\n","<td>Etc/UTC</td></tr>\n","<tr><td>H2O_data_parsing_timezone:</td>\n","<td>UTC</td></tr>\n","<tr><td>H2O_cluster_version:</td>\n","<td>3.32.1.6</td></tr>\n","<tr><td>H2O_cluster_version_age:</td>\n","<td>3 days </td></tr>\n","<tr><td>H2O_cluster_name:</td>\n","<td>H2O_from_python_unknownUser_s24n8u</td></tr>\n","<tr><td>H2O_cluster_total_nodes:</td>\n","<td>1</td></tr>\n","<tr><td>H2O_cluster_free_memory:</td>\n","<td>3.172 Gb</td></tr>\n","<tr><td>H2O_cluster_total_cores:</td>\n","<td>2</td></tr>\n","<tr><td>H2O_cluster_allowed_cores:</td>\n","<td>2</td></tr>\n","<tr><td>H2O_cluster_status:</td>\n","<td>locked, healthy</td></tr>\n","<tr><td>H2O_connection_url:</td>\n","<td>http://localhost:54321</td></tr>\n","<tr><td>H2O_connection_proxy:</td>\n","<td>{\"http\": null, \"https\": null}</td></tr>\n","<tr><td>H2O_internal_security:</td>\n","<td>False</td></tr>\n","<tr><td>H2O_API_Extensions:</td>\n","<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n","<tr><td>Python_version:</td>\n","<td>3.7.11 final</td></tr></table></div>"],"text/plain":["--------------------------  ------------------------------------------------------------------\n","H2O_cluster_uptime:         53 mins 02 secs\n","H2O_cluster_timezone:       Etc/UTC\n","H2O_data_parsing_timezone:  UTC\n","H2O_cluster_version:        3.32.1.6\n","H2O_cluster_version_age:    3 days\n","H2O_cluster_name:           H2O_from_python_unknownUser_s24n8u\n","H2O_cluster_total_nodes:    1\n","H2O_cluster_free_memory:    3.172 Gb\n","H2O_cluster_total_cores:    2\n","H2O_cluster_allowed_cores:  2\n","H2O_cluster_status:         locked, healthy\n","H2O_connection_url:         http://localhost:54321\n","H2O_connection_proxy:       {\"http\": null, \"https\": null}\n","H2O_internal_security:      False\n","H2O_API_Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n","Python_version:             3.7.11 final\n","--------------------------  ------------------------------------------------------------------"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"RQVI-ISXvrZm"},"source":["#**************************LOADING DATASET*******************************"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FEGPULDrvk3g","executionInfo":{"status":"ok","timestamp":1629682646356,"user_tz":-570,"elapsed":1222,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"3091aac4-a12c-47a4-8cad-5b3c1ebc7f55"},"source":["from google.colab import drive \n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qDh3f5HwHubq"},"source":["# PR REMOVER 1\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uN9VfZBAvxCj","executionInfo":{"status":"ok","timestamp":1629683707029,"user_tz":-570,"elapsed":1060680,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"e7cf9ae8-3be8-4f73-d7d9-570d8e6a0729"},"source":["for i in range(1,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","\n","  # # normalization of train and test sets\n","  # Fitter= MM.fit(train)\n","  # transformed_train=Fitter.transform(train)\n","  # train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  # #test normalization\n","  # transformed_test=Fitter.transform(test)\n","  # test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","  #BLD Train set\n","  class Train(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Train, self).__init__(df=train  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","  # BLD Test\n","  BLD_Train = Train(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","  #******************************** prejudice remover regularizer*****************************\n","  Classifier = PrejudiceRemover(eta= 1, sensitive_attr= 'race')\n","  Classifier.fit(BLD_Train)\n","  prediction= Classifier.predict(BLD_Test)\n","  #**********************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS****************************\n","  #predicted labels\n","  # gbm_Predictions= best_model.predict(Test)\n","  # gbm_Predictions= gbm_Predictions.as_data_frame()\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= prediction.labels\n","\n","  # ********************COMPUTE DISCRIMINATION*****************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","\n","  excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover1.xlsx')\n","  Violent= excelBook['Violent']\n","  data= Violent.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n","  10# Create a DataFrame based on the second and subsequent lines of data\n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover1.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Violent', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Accuracy 0.830423940149626\n","Accuracy 0.8428927680798005\n","Accuracy 0.8478802992518704\n","Accuracy 0.8341645885286783\n","Accuracy 0.844139650872818\n","Accuracy 0.830423940149626\n","Accuracy 0.816708229426434\n","Accuracy 0.85785536159601\n","Accuracy 0.8403990024937655\n","Accuracy 0.8591022443890274\n","Accuracy 0.844139650872818\n","Accuracy 0.8291770573566085\n","Accuracy 0.8466334164588528\n","Accuracy 0.8428927680798005\n","Accuracy 0.8329177057356608\n","Accuracy 0.8391521197007481\n","Accuracy 0.8516209476309227\n","Accuracy 0.8379052369077307\n","Accuracy 0.830423940149626\n","Accuracy 0.8403990024937655\n","Accuracy 0.85785536159601\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8129675810473815\n","Accuracy 0.8403990024937655\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8416458852867831\n","Accuracy 0.8428927680798005\n","Accuracy 0.8354114713216958\n","Accuracy 0.8341645885286783\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8553615960099751\n","Accuracy 0.8204488778054863\n","Accuracy 0.8553615960099751\n","Accuracy 0.8428927680798005\n","Accuracy 0.844139650872818\n","Accuracy 0.8478802992518704\n","Accuracy 0.8291770573566085\n","Accuracy 0.830423940149626\n","Accuracy 0.8478802992518704\n","Accuracy 0.8416458852867831\n","Accuracy 0.8316708229426434\n","Accuracy 0.8379052369077307\n","Accuracy 0.844139650872818\n","Accuracy 0.8416458852867831\n","Accuracy 0.8528678304239401\n","Accuracy 0.827930174563591\n","Accuracy 0.8366583541147132\n","Accuracy 0.8391521197007481\n","Accuracy 0.8391521197007481\n","Accuracy 0.8229426433915212\n","Accuracy 0.8403990024937655\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8491271820448878\n","Accuracy 0.8391521197007481\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4H3vvxWBboi5"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"reSfNpjhFH8r"},"source":["# PR REMOVER 25\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dtplOkDrFH8r","executionInfo":{"status":"ok","timestamp":1629684825941,"user_tz":-570,"elapsed":1118944,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"1a16b71d-ccfe-4d19-8adb-cf43ccd88fef"},"source":["for i in range(1,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","\n","  # # normalization of train and test sets\n","  # Fitter= MM.fit(train)\n","  # transformed_train=Fitter.transform(train)\n","  # train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  # #test normalization\n","  # transformed_test=Fitter.transform(test)\n","  # test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","  #BLD Train set\n","  class Train(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Train, self).__init__(df=train  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","  # BLD Test\n","  BLD_Train = Train(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","  #******************************** prejudice remover regularizer*****************************\n","  Classifier = PrejudiceRemover(eta= 25, sensitive_attr= 'race')\n","  Classifier.fit(BLD_Train)\n","  prediction= Classifier.predict(BLD_Test)\n","  #**********************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS****************************\n","  #predicted labels\n","  # gbm_Predictions= best_model.predict(Test)\n","  # gbm_Predictions= gbm_Predictions.as_data_frame()\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= prediction.labels\n","\n","  # ********************COMPUTE DISCRIMINATION*****************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","\n","  excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover25.xlsx')\n","  Violent= excelBook['Violent']\n","  data= Violent.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n","  10# Create a DataFrame based on the second and subsequent lines of data\n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover25.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Violent', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Accuracy 0.8316708229426434\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8428927680798005\n","Accuracy 0.8453865336658354\n","Accuracy 0.8341645885286783\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8379052369077307\n","Accuracy 0.8329177057356608\n","Accuracy 0.816708229426434\n","Accuracy 0.8528678304239401\n","Accuracy 0.8379052369077307\n","Accuracy 0.8553615960099751\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8516209476309227\n","Accuracy 0.830423940149626\n","Accuracy 0.8453865336658354\n","Accuracy 0.844139650872818\n","Accuracy 0.8329177057356608\n","Accuracy 0.8379052369077307\n","Accuracy 0.8528678304239401\n","Accuracy 0.8341645885286783\n","Accuracy 0.8291770573566085\n","Accuracy 0.8403990024937655\n","Accuracy 0.8528678304239401\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8129675810473815\n","Accuracy 0.8391521197007481\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8453865336658354\n","Accuracy 0.8428927680798005\n","Accuracy 0.8291770573566085\n","Accuracy 0.8329177057356608\n","Accuracy 0.8541147132169576\n","Accuracy 0.8266832917705735\n","Accuracy 0.8491271820448878\n","Accuracy 0.844139650872818\n","Accuracy 0.8466334164588528\n","Accuracy 0.8478802992518704\n","Accuracy 0.8291770573566085\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8316708229426434\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8466334164588528\n","Accuracy 0.8416458852867831\n","Accuracy 0.8329177057356608\n","Accuracy 0.8391521197007481\n","Accuracy 0.844139650872818\n","Accuracy 0.8416458852867831\n","Accuracy 0.8553615960099751\n","Accuracy 0.8254364089775561\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8354114713216958\n","Accuracy 0.8391521197007481\n","Accuracy 0.8428927680798005\n","Accuracy 0.8192019950124688\n","Accuracy 0.8391521197007481\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8478802992518704\n","Accuracy 0.8341645885286783\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2zGRNHEQFIPE"},"source":["# PR REMOVER 50\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KOUNqFT4FIPG","executionInfo":{"status":"ok","timestamp":1629685893867,"user_tz":-570,"elapsed":1067943,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"27248a6c-688d-49ea-f008-34abda80cbf9"},"source":["for i in range(1,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","\n","  # # normalization of train and test sets\n","  # Fitter= MM.fit(train)\n","  # transformed_train=Fitter.transform(train)\n","  # train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  # #test normalization\n","  # transformed_test=Fitter.transform(test)\n","  # test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","  #BLD Train set\n","  class Train(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Train, self).__init__(df=train  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","  # BLD Test\n","  BLD_Train = Train(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","  #******************************** prejudice remover regularizer*****************************\n","  Classifier = PrejudiceRemover(eta= 50, sensitive_attr= 'race')\n","  Classifier.fit(BLD_Train)\n","  prediction= Classifier.predict(BLD_Test)\n","  #**********************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS****************************\n","  #predicted labels\n","  # gbm_Predictions= best_model.predict(Test)\n","  # gbm_Predictions= gbm_Predictions.as_data_frame()\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= prediction.labels\n","\n","  # ********************COMPUTE DISCRIMINATION*****************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","\n","  excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover50.xlsx')\n","  Violent= excelBook['Violent']\n","  data= Violent.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n","  10# Create a DataFrame based on the second and subsequent lines of data\n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover50.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Violent', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Accuracy 0.830423940149626\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8453865336658354\n","Accuracy 0.8453865336658354\n","Accuracy 0.8354114713216958\n","Accuracy 0.8379052369077307\n","Accuracy 0.8329177057356608\n","Accuracy 0.8179551122194514\n","Accuracy 0.8491271820448878\n","Accuracy 0.8354114713216958\n","Accuracy 0.8541147132169576\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8528678304239401\n","Accuracy 0.8341645885286783\n","Accuracy 0.8453865336658354\n","Accuracy 0.844139650872818\n","Accuracy 0.8329177057356608\n","Accuracy 0.8379052369077307\n","Accuracy 0.8528678304239401\n","Accuracy 0.8329177057356608\n","Accuracy 0.8316708229426434\n","Accuracy 0.8403990024937655\n","Accuracy 0.8528678304239401\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8129675810473815\n","Accuracy 0.8403990024937655\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8466334164588528\n","Accuracy 0.844139650872818\n","Accuracy 0.827930174563591\n","Accuracy 0.8329177057356608\n","Accuracy 0.8528678304239401\n","Accuracy 0.8291770573566085\n","Accuracy 0.8491271820448878\n","Accuracy 0.8466334164588528\n","Accuracy 0.844139650872818\n","Accuracy 0.8478802992518704\n","Accuracy 0.8291770573566085\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.827930174563591\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8478802992518704\n","Accuracy 0.8416458852867831\n","Accuracy 0.8316708229426434\n","Accuracy 0.8354114713216958\n","Accuracy 0.8416458852867831\n","Accuracy 0.8403990024937655\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8553615960099751\n","Accuracy 0.8254364089775561\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8341645885286783\n","Accuracy 0.8516209476309227\n","Accuracy 0.8428927680798005\n","Accuracy 0.8192019950124688\n","Accuracy 0.8416458852867831\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8466334164588528\n","Accuracy 0.8316708229426434\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SglOigoJKEiy"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"vP-IlZfPKFU6"},"source":["# PR REMOVER 75\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N4pHADeEKFU7","executionInfo":{"status":"ok","timestamp":1629687007013,"user_tz":-570,"elapsed":1113163,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"b8e7e9d8-93a5-43d1-8c09-912ee2f695d7"},"source":["for i in range(1,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","\n","  # # normalization of train and test sets\n","  # Fitter= MM.fit(train)\n","  # transformed_train=Fitter.transform(train)\n","  # train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  # #test normalization\n","  # transformed_test=Fitter.transform(test)\n","  # test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","  #BLD Train set\n","  class Train(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Train, self).__init__(df=train  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","  # BLD Test\n","  BLD_Train = Train(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","  #******************************** prejudice remover regularizer*****************************\n","  Classifier = PrejudiceRemover(eta= 75, sensitive_attr= 'race')\n","  Classifier.fit(BLD_Train)\n","  prediction= Classifier.predict(BLD_Test)\n","  #**********************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS****************************\n","  #predicted labels\n","  # gbm_Predictions= best_model.predict(Test)\n","  # gbm_Predictions= gbm_Predictions.as_data_frame()\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= prediction.labels\n","\n","  # ********************COMPUTE DISCRIMINATION*****************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","\n","  excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover75.xlsx')\n","  Violent= excelBook['Violent']\n","  data= Violent.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n","  10# Create a DataFrame based on the second and subsequent lines of data\n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover75.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Violent', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Accuracy 0.8316708229426434\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8428927680798005\n","Accuracy 0.8453865336658354\n","Accuracy 0.8366583541147132\n","Accuracy 0.8391521197007481\n","Accuracy 0.8341645885286783\n","Accuracy 0.8192019950124688\n","Accuracy 0.8503740648379052\n","Accuracy 0.8329177057356608\n","Accuracy 0.8528678304239401\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8516209476309227\n","Accuracy 0.8341645885286783\n","Accuracy 0.8503740648379052\n","Accuracy 0.844139650872818\n","Accuracy 0.8329177057356608\n","Accuracy 0.8379052369077307\n","Accuracy 0.8528678304239401\n","Accuracy 0.8316708229426434\n","Accuracy 0.8316708229426434\n","Accuracy 0.8403990024937655\n","Accuracy 0.8528678304239401\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8129675810473815\n","Accuracy 0.8391521197007481\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8466334164588528\n","Accuracy 0.844139650872818\n","Accuracy 0.827930174563591\n","Accuracy 0.8316708229426434\n","Accuracy 0.8528678304239401\n","Accuracy 0.8316708229426434\n","Accuracy 0.8491271820448878\n","Accuracy 0.8466334164588528\n","Accuracy 0.844139650872818\n","Accuracy 0.8478802992518704\n","Accuracy 0.8291770573566085\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.827930174563591\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8453865336658354\n","Accuracy 0.8416458852867831\n","Accuracy 0.827930174563591\n","Accuracy 0.8254364089775561\n","Accuracy 0.8416458852867831\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8416458852867831\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8516209476309227\n","Accuracy 0.827930174563591\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8341645885286783\n","Accuracy 0.8516209476309227\n","Accuracy 0.8428927680798005\n","Accuracy 0.8179551122194514\n"],"name":"stdout"},{"output_type":"stream","text":["invalid value encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8416458852867831\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8491271820448878\n","Accuracy 0.8291770573566085\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NDxf1UeRVgOp"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"eNlkgkHyKHkO"},"source":["# PR REMOVER 100\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"25xsmsZAKHkS","executionInfo":{"status":"ok","timestamp":1629688098767,"user_tz":-570,"elapsed":1091783,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"ad99cf4c-59f3-4abc-a5e6-cd604891206c"},"source":["for i in range(1,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","\n","  # # normalization of train and test sets\n","  # Fitter= MM.fit(train)\n","  # transformed_train=Fitter.transform(train)\n","  # train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  # #test normalization\n","  # transformed_test=Fitter.transform(test)\n","  # test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","  #BLD Train set\n","  class Train(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Train, self).__init__(df=train  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","  # BLD Test\n","  BLD_Train = Train(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","  #******************************** prejudice remover regularizer*****************************\n","  Classifier = PrejudiceRemover(eta= 100, sensitive_attr= 'race')\n","  Classifier.fit(BLD_Train)\n","  prediction= Classifier.predict(BLD_Test)\n","  #**********************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS****************************\n","  #predicted labels\n","  # gbm_Predictions= best_model.predict(Test)\n","  # gbm_Predictions= gbm_Predictions.as_data_frame()\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= prediction.labels\n","\n","  # ********************COMPUTE DISCRIMINATION*****************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","\n","  excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover100.xlsx')\n","  Violent= excelBook['Violent']\n","  data= Violent.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n","  10# Create a DataFrame based on the second and subsequent lines of data\n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover100.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Violent', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Accuracy 0.8316708229426434\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8428927680798005\n","Accuracy 0.844139650872818\n","Accuracy 0.8341645885286783\n","Accuracy 0.8391521197007481\n","Accuracy 0.8341645885286783\n","Accuracy 0.8192019950124688\n","Accuracy 0.8503740648379052\n","Accuracy 0.8329177057356608\n","Accuracy 0.8541147132169576\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8516209476309227\n","Accuracy 0.8291770573566085\n","Accuracy 0.8391521197007481\n","Accuracy 0.844139650872818\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8416458852867831\n","Accuracy 0.8379052369077307\n","Accuracy 0.8528678304239401\n","Accuracy 0.830423940149626\n","Accuracy 0.8316708229426434\n","Accuracy 0.8403990024937655\n","Accuracy 0.8528678304239401\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8129675810473815\n","Accuracy 0.8366583541147132\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8466334164588528\n","Accuracy 0.8416458852867831\n","Accuracy 0.8291770573566085\n","Accuracy 0.8316708229426434\n","Accuracy 0.8528678304239401\n","Accuracy 0.8316708229426434\n","Accuracy 0.8491271820448878\n","Accuracy 0.8466334164588528\n","Accuracy 0.844139650872818\n","Accuracy 0.8453865336658354\n","Accuracy 0.8291770573566085\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8266832917705735\n"],"name":"stdout"},{"output_type":"stream","text":["invalid value encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.844139650872818\n","Accuracy 0.8416458852867831\n","Accuracy 0.8266832917705735\n","Accuracy 0.8366583541147132\n","Accuracy 0.8416458852867831\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.844139650872818\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8503740648379052\n","Accuracy 0.8254364089775561\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.830423940149626\n","Accuracy 0.8516209476309227\n","Accuracy 0.8453865336658354\n","Accuracy 0.8179551122194514\n"],"name":"stdout"},{"output_type":"stream","text":["invalid value encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8416458852867831\n"],"name":"stdout"},{"output_type":"stream","text":["divide by zero encountered in double_scalars\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy 0.8491271820448878\n","Accuracy 0.8291770573566085\n"],"name":"stdout"}]}]}