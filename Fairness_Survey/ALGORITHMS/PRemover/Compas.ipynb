{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Compas.ipynb","provenance":[{"file_id":"1RyrIFd1EbO7wTWXg1693zq8SahcbsU8L","timestamp":1628685738191},{"file_id":"1zUVqlZwnp2gUWV_U6552PNihQarJC9I9","timestamp":1628683226587},{"file_id":"1AJWD--4mn4SFR1x2nX38Hkscv0C2Ovqi","timestamp":1628499839227}],"authorship_tag":"ABX9TyNDH5vhidlCX4r5A7Pbv0T0"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ff_Jv8Ptu4lP"},"source":["\n","# INSTALLATION"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q5qYRG8zufHw","executionInfo":{"status":"ok","timestamp":1629679411091,"user_tz":-570,"elapsed":14567,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"003edc6b-0ab7-4944-c326-843d1d889ddc"},"source":["!pip install aif360\n","!pip install fairlearn"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting aif360\n","  Downloading aif360-0.4.0-py3-none-any.whl (175 kB)\n","\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 175 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.1.5)\n","Requirement already satisfied: scipy<1.6.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.4.1)\n","Collecting tempeh\n","  Downloading tempeh-0.1.12-py3-none-any.whl (39 kB)\n","Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from aif360) (0.22.2.post1)\n","Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.19.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from aif360) (3.2.2)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->aif360) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->aif360) (1.0.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (1.3.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (0.10.0)\n","Collecting memory-profiler\n","  Downloading memory_profiler-0.58.0.tar.gz (36 kB)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360) (3.6.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360) (2.23.0)\n","Collecting shap\n","  Downloading shap-0.39.0.tar.gz (356 kB)\n","\u001b[K     |████████████████████████████████| 356 kB 57.6 MB/s \n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler->tempeh->aif360) (5.4.8)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (0.7.1)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (1.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (57.4.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (8.8.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (21.2.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (1.10.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (3.0.4)\n","Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360) (4.62.0)\n","Collecting slicer==0.0.7\n","  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n","Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360) (0.51.2)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360) (1.3.0)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap->tempeh->aif360) (0.34.0)\n","Building wheels for collected packages: memory-profiler, shap\n","  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for memory-profiler: filename=memory_profiler-0.58.0-py3-none-any.whl size=30190 sha256=f2279d7a08f668ded5901c4bc1e42d6d4c8ebd0f5a0f49e8c3e5db23525ed825\n","  Stored in directory: /root/.cache/pip/wheels/56/19/d5/8cad06661aec65a04a0d6785b1a5ad035cb645b1772a4a0882\n","  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for shap: filename=shap-0.39.0-cp37-cp37m-linux_x86_64.whl size=491635 sha256=1192296ac3a63260adde6b0f706897a47a2d3e8d9963f3e665bbe69ff8434a2a\n","  Stored in directory: /root/.cache/pip/wheels/ca/25/8f/6ae5df62c32651cd719e972e738a8aaa4a87414c4d2b14c9c0\n","Successfully built memory-profiler shap\n","Installing collected packages: slicer, shap, memory-profiler, tempeh, aif360\n","Successfully installed aif360-0.4.0 memory-profiler-0.58.0 shap-0.39.0 slicer-0.0.7 tempeh-0.1.12\n","Collecting fairlearn\n","  Downloading fairlearn-0.7.0-py3-none-any.whl (177 kB)\n","\u001b[K     |████████████████████████████████| 177 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (0.22.2.post1)\n","Requirement already satisfied: pandas>=0.25.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (1.1.5)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (1.4.1)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (1.19.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.1->fairlearn) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.1->fairlearn) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25.1->fairlearn) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->fairlearn) (1.0.1)\n","Installing collected packages: fairlearn\n","Successfully installed fairlearn-0.7.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TltW3iPkux0Q","executionInfo":{"status":"ok","timestamp":1629679412242,"user_tz":-570,"elapsed":1157,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"4a02af96-2b30-4cc3-fb6b-d747f4db3e8f"},"source":["!apt-get install -jre\n","!java -version"],"execution_count":2,"outputs":[{"output_type":"stream","text":["E: Command line option 'j' [from -jre] is not understood in combination with the other options.\n","openjdk version \"11.0.11\" 2021-04-20\n","OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04)\n","OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KssrNl8GvDYU","executionInfo":{"status":"ok","timestamp":1629679446243,"user_tz":-570,"elapsed":34010,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"72eaeec0-6772-4959-d03b-738c8c4813eb"},"source":["!pip install h2o"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting h2o\n","  Downloading h2o-3.32.1.6.tar.gz (168.4 MB)\n","\u001b[K     |████████████████████████████████| 168.4 MB 21 kB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from h2o) (2.23.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from h2o) (0.8.9)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from h2o) (0.16.0)\n","Collecting colorama>=0.3.8\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (2.10)\n","Building wheels for collected packages: h2o\n","  Building wheel for h2o (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for h2o: filename=h2o-3.32.1.6-py2.py3-none-any.whl size=168439194 sha256=80718e460435c3824fa3e6bfbecf8f4bc25678c1bff733d30f86d1e4f86df854\n","  Stored in directory: /root/.cache/pip/wheels/ee/0f/51/849ba221c4c1b11a04efb4a3427dc9cb1c4dcde218c6c98b13\n","Successfully built h2o\n","Installing collected packages: colorama, h2o\n","Successfully installed colorama-0.4.4 h2o-3.32.1.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_NQn2JJ0uw6u","executionInfo":{"status":"ok","timestamp":1629679449011,"user_tz":-570,"elapsed":2784,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"cead4632-2f9e-447f-cea2-f6cdd82dc48b"},"source":["!pip install xlsxwriter"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting xlsxwriter\n","  Downloading XlsxWriter-3.0.1-py3-none-any.whl (148 kB)\n","\u001b[?25l\r\u001b[K     |██▏                             | 10 kB 26.0 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 20 kB 32.7 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 30 kB 15.5 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 40 kB 11.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 71 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 81 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 148 kB 5.1 MB/s \n","\u001b[?25hInstalling collected packages: xlsxwriter\n","Successfully installed xlsxwriter-3.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0YklbHpAxd8","executionInfo":{"status":"ok","timestamp":1629679454512,"user_tz":-570,"elapsed":5507,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"28a295bf-3dfa-42a3-a5f3-fac670242dca"},"source":["!pip install BlackBoxAuditing"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting BlackBoxAuditing\n","  Downloading BlackBoxAuditing-0.1.54.tar.gz (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (2.6.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (3.2.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (1.1.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (1.19.5)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (0.10.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->BlackBoxAuditing) (1.15.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->BlackBoxAuditing) (2018.9)\n","Building wheels for collected packages: BlackBoxAuditing\n","  Building wheel for BlackBoxAuditing (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for BlackBoxAuditing: filename=BlackBoxAuditing-0.1.54-py2.py3-none-any.whl size=1394771 sha256=c854c0539cc522fde8f5c4f29adb7bb3b58fb9ffef51b8c89aefc2231126b98f\n","  Stored in directory: /root/.cache/pip/wheels/05/9f/ee/541a74be4cf5dad17430e64d3276370ea7b6a834a76cb4215a\n","Successfully built BlackBoxAuditing\n","Installing collected packages: BlackBoxAuditing\n","Successfully installed BlackBoxAuditing-0.1.54\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-Y_uQ6vdvN4a"},"source":["#IMPORTS"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rf1aISz6vGfR","executionInfo":{"status":"ok","timestamp":1629679465071,"user_tz":-570,"elapsed":10568,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"919741bd-af35-4945-8a54-e270bbd088f9"},"source":["import numpy as np\n","from mlxtend.feature_selection import  ExhaustiveFeatureSelector\n","from xgboost import  XGBClassifier\n","# import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import openpyxl\n","import xlsxwriter\n","from openpyxl import  load_workbook\n","\n","import BlackBoxAuditing\n","import shap\n","#suppress setwith copy warning\n","pd.set_option('mode.chained_assignment',None)\n","from sklearn.feature_selection import VarianceThreshold\n","from sklearn.feature_selection import SelectKBest, SelectFwe, SelectPercentile,SelectFdr, SelectFpr, SelectFromModel\n","from sklearn.feature_selection import chi2, mutual_info_classif\n","# from skfeature.function.similarity_based import fisher_score\n","from aif360.algorithms.inprocessing import PrejudiceRemover\n","import matplotlib.pyplot as plt\n","from aif360.metrics.classification_metric import ClassificationMetric\n","\n","from aif360.metrics import BinaryLabelDatasetMetric\n","from aif360.algorithms.preprocessing import DisparateImpactRemover, Reweighing, LFR,OptimPreproc\n","from aif360.datasets import StandardDataset , BinaryLabelDataset\n","from sklearn.preprocessing import MinMaxScaler \n","MM= MinMaxScaler()\n","import h2o\n","from h2o.automl import H2OAutoML\n","from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n","\n","import sys\n","sys.path.append(\"../\")\n","import os\n","\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":554},"id":"RcxQeeX7vUXz","executionInfo":{"status":"ok","timestamp":1629679471820,"user_tz":-570,"elapsed":6770,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"47b9856f-eaf4-44fc-e34d-859bf9ed242f"},"source":["h2o.init()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n","Attempting to start a local H2O server...\n","  Java Version: openjdk version \"11.0.11\" 2021-04-20; OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04); OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing)\n","  Starting server from /usr/local/lib/python3.7/dist-packages/h2o/backend/bin/h2o.jar\n","  Ice root: /tmp/tmpkzhwc8yz\n","  JVM stdout: /tmp/tmpkzhwc8yz/h2o_unknownUser_started_from_python.out\n","  JVM stderr: /tmp/tmpkzhwc8yz/h2o_unknownUser_started_from_python.err\n","  Server is running at http://127.0.0.1:54321\n","Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n","<td>03 secs</td></tr>\n","<tr><td>H2O_cluster_timezone:</td>\n","<td>Etc/UTC</td></tr>\n","<tr><td>H2O_data_parsing_timezone:</td>\n","<td>UTC</td></tr>\n","<tr><td>H2O_cluster_version:</td>\n","<td>3.32.1.6</td></tr>\n","<tr><td>H2O_cluster_version_age:</td>\n","<td>3 days </td></tr>\n","<tr><td>H2O_cluster_name:</td>\n","<td>H2O_from_python_unknownUser_4ct8xh</td></tr>\n","<tr><td>H2O_cluster_total_nodes:</td>\n","<td>1</td></tr>\n","<tr><td>H2O_cluster_free_memory:</td>\n","<td>3.172 Gb</td></tr>\n","<tr><td>H2O_cluster_total_cores:</td>\n","<td>2</td></tr>\n","<tr><td>H2O_cluster_allowed_cores:</td>\n","<td>2</td></tr>\n","<tr><td>H2O_cluster_status:</td>\n","<td>accepting new members, healthy</td></tr>\n","<tr><td>H2O_connection_url:</td>\n","<td>http://127.0.0.1:54321</td></tr>\n","<tr><td>H2O_connection_proxy:</td>\n","<td>{\"http\": null, \"https\": null}</td></tr>\n","<tr><td>H2O_internal_security:</td>\n","<td>False</td></tr>\n","<tr><td>H2O_API_Extensions:</td>\n","<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n","<tr><td>Python_version:</td>\n","<td>3.7.11 final</td></tr></table></div>"],"text/plain":["--------------------------  ------------------------------------------------------------------\n","H2O_cluster_uptime:         03 secs\n","H2O_cluster_timezone:       Etc/UTC\n","H2O_data_parsing_timezone:  UTC\n","H2O_cluster_version:        3.32.1.6\n","H2O_cluster_version_age:    3 days\n","H2O_cluster_name:           H2O_from_python_unknownUser_4ct8xh\n","H2O_cluster_total_nodes:    1\n","H2O_cluster_free_memory:    3.172 Gb\n","H2O_cluster_total_cores:    2\n","H2O_cluster_allowed_cores:  2\n","H2O_cluster_status:         accepting new members, healthy\n","H2O_connection_url:         http://127.0.0.1:54321\n","H2O_connection_proxy:       {\"http\": null, \"https\": null}\n","H2O_internal_security:      False\n","H2O_API_Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n","Python_version:             3.7.11 final\n","--------------------------  ------------------------------------------------------------------"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"RQVI-ISXvrZm"},"source":["#**************************LOADING DATASET*******************************"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FEGPULDrvk3g","executionInfo":{"status":"ok","timestamp":1629681927400,"user_tz":-570,"elapsed":1371,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"200a8063-49a2-4f3f-8e78-de496e031d4a"},"source":["from google.colab import drive \n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qDh3f5HwHubq"},"source":["# PR REMOVER 1\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uN9VfZBAvxCj","executionInfo":{"status":"ok","timestamp":1629683600865,"user_tz":-570,"elapsed":1673469,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"303661d9-9fb5-4fb1-aa89-adccf8d17669"},"source":["for i in range(1,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Compas/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Compas/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","\n","  # # normalization of train and test sets\n","  # Fitter= MM.fit(train)\n","  # transformed_train=Fitter.transform(train)\n","  # train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  # #test normalization\n","  # transformed_test=Fitter.transform(test)\n","  # test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","  #BLD Train set\n","  class Train(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Train, self).__init__(df=train  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","  # BLD Test\n","  BLD_Train = Train(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","  #******************************** prejudice remover regularizer*****************************\n","  Classifier = PrejudiceRemover(eta= 1, sensitive_attr= 'race')\n","  Classifier.fit(BLD_Train)\n","  prediction= Classifier.predict(BLD_Test)\n","  #**********************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS****************************\n","  #predicted labels\n","  # gbm_Predictions= best_model.predict(Test)\n","  # gbm_Predictions= gbm_Predictions.as_data_frame()\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= prediction.labels\n","\n","  # ********************COMPUTE DISCRIMINATION*****************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","\n","  excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover1.xlsx')\n","  Compas= excelBook['Compas']\n","  data= Compas.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n","  10# Create a DataFrame based on the second and subsequent lines of data\n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover1.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Compas', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Accuracy 0.660453808752026\n","Accuracy 0.6985413290113452\n","Accuracy 0.6763990267639902\n","Accuracy 0.6772100567721006\n","Accuracy 0.6536901865369019\n","Accuracy 0.6758508914100486\n","Accuracy 0.6709886547811994\n","Accuracy 0.6731549067315491\n","Accuracy 0.6609894566098946\n","Accuracy 0.67558799675588\n","Accuracy 0.6790923824959482\n","Accuracy 0.7025931928687196\n","Accuracy 0.6634225466342255\n","Accuracy 0.6772100567721006\n","Accuracy 0.67558799675588\n","Accuracy 0.6839546191247974\n","Accuracy 0.6620745542949756\n","Accuracy 0.6731549067315491\n","Accuracy 0.6828872668288727\n","Accuracy 0.6634225466342255\n","Accuracy 0.6839546191247974\n","Accuracy 0.6515397082658023\n","Accuracy 0.6642335766423357\n","Accuracy 0.6780210867802109\n","Accuracy 0.6853203568532036\n","Accuracy 0.6823338735818476\n","Accuracy 0.679902755267423\n","Accuracy 0.6642335766423357\n","Accuracy 0.7015409570154095\n","Accuracy 0.667477696674777\n","Accuracy 0.6904376012965965\n","Accuracy 0.6677471636952999\n","Accuracy 0.6585563665855637\n","Accuracy 0.6853203568532036\n","Accuracy 0.6763990267639902\n","Accuracy 0.6693679092382496\n","Accuracy 0.6750405186385737\n","Accuracy 0.6650446066504461\n","Accuracy 0.6918085969180859\n","Accuracy 0.681265206812652\n","Accuracy 0.6726094003241491\n","Accuracy 0.6871961102106969\n","Accuracy 0.6974858069748581\n","Accuracy 0.6536901865369019\n","Accuracy 0.6772100567721006\n","Accuracy 0.6620745542949756\n","Accuracy 0.6823338735818476\n","Accuracy 0.681265206812652\n","Accuracy 0.6666666666666666\n","Accuracy 0.6723438767234388\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4H3vvxWBboi5"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"reSfNpjhFH8r"},"source":["# PR REMOVER 25\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dtplOkDrFH8r","executionInfo":{"status":"ok","timestamp":1629687503343,"user_tz":-570,"elapsed":1953386,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"d50a3fd2-94c9-4ada-ef31-4b09da41132f"},"source":["for i in range(1,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Compas/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Compas/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","\n","  # # normalization of train and test sets\n","  # Fitter= MM.fit(train)\n","  # transformed_train=Fitter.transform(train)\n","  # train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  # #test normalization\n","  # transformed_test=Fitter.transform(test)\n","  # test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","  #BLD Train set\n","  class Train(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Train, self).__init__(df=train  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","  # BLD Test\n","  BLD_Train = Train(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","  #******************************** prejudice remover regularizer*****************************\n","  Classifier = PrejudiceRemover(eta= 25, sensitive_attr= 'race')\n","  Classifier.fit(BLD_Train)\n","  prediction= Classifier.predict(BLD_Test)\n","  #**********************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS****************************\n","  #predicted labels\n","  # gbm_Predictions= best_model.predict(Test)\n","  # gbm_Predictions= gbm_Predictions.as_data_frame()\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= prediction.labels\n","\n","  # ********************COMPUTE DISCRIMINATION*****************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","\n","  excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover25.xlsx')\n","  Compas= excelBook['Compas']\n","  data= Compas.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n","  10# Create a DataFrame based on the second and subsequent lines of data\n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover25.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Compas', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Accuracy 0.6612641815235009\n","Accuracy 0.6839546191247974\n","Accuracy 0.667477696674777\n","Accuracy 0.6739659367396593\n","Accuracy 0.6472019464720195\n","Accuracy 0.6628849270664505\n","Accuracy 0.666936790923825\n","Accuracy 0.6682887266828873\n","Accuracy 0.6634225466342255\n","Accuracy 0.6763990267639902\n","Accuracy 0.673419773095624\n","Accuracy 0.6896272285251216\n","Accuracy 0.656934306569343\n","Accuracy 0.6666666666666666\n","Accuracy 0.6707218167072182\n","Accuracy 0.6693679092382496\n","Accuracy 0.6564019448946515\n","Accuracy 0.6699107866991079\n","Accuracy 0.6877534468775345\n","Accuracy 0.6601784266017843\n","Accuracy 0.6774716369529984\n","Accuracy 0.6450567260940032\n","Accuracy 0.656934306569343\n","Accuracy 0.6699107866991079\n","Accuracy 0.6845093268450932\n","Accuracy 0.6693679092382496\n","Accuracy 0.6766612641815235\n","Accuracy 0.6634225466342255\n","Accuracy 0.6934306569343066\n","Accuracy 0.6658556366585564\n","Accuracy 0.6912479740680713\n","Accuracy 0.6653160453808752\n","Accuracy 0.6496350364963503\n","Accuracy 0.6731549067315491\n","Accuracy 0.6658556366585564\n","Accuracy 0.6653160453808752\n","Accuracy 0.6645056726094003\n","Accuracy 0.6593673965936739\n","Accuracy 0.6934306569343066\n","Accuracy 0.6731549067315491\n","Accuracy 0.6758508914100486\n","Accuracy 0.6847649918962723\n","Accuracy 0.6869424168694241\n","Accuracy 0.6553122465531225\n","Accuracy 0.667477696674777\n","Accuracy 0.6555915721231766\n","Accuracy 0.673419773095624\n","Accuracy 0.6845093268450932\n","Accuracy 0.6545012165450121\n","Accuracy 0.6699107866991079\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2zGRNHEQFIPE"},"source":["# PR REMOVER 50\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KOUNqFT4FIPG","executionInfo":{"status":"ok","timestamp":1629689126031,"user_tz":-570,"elapsed":1622728,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"317c7b16-adf4-403a-de68-1236f0990279"},"source":["for i in range(1,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Compas/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Compas/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","\n","  # # normalization of train and test sets\n","  # Fitter= MM.fit(train)\n","  # transformed_train=Fitter.transform(train)\n","  # train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  # #test normalization\n","  # transformed_test=Fitter.transform(test)\n","  # test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","  #BLD Train set\n","  class Train(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Train, self).__init__(df=train  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","  # BLD Test\n","  BLD_Train = Train(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","  #******************************** prejudice remover regularizer*****************************\n","  Classifier = PrejudiceRemover(eta= 50, sensitive_attr= 'race')\n","  Classifier.fit(BLD_Train)\n","  prediction= Classifier.predict(BLD_Test)\n","  #**********************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS****************************\n","  #predicted labels\n","  # gbm_Predictions= best_model.predict(Test)\n","  # gbm_Predictions= gbm_Predictions.as_data_frame()\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= prediction.labels\n","\n","  # ********************COMPUTE DISCRIMINATION*****************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","\n","  excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover50.xlsx')\n","  Compas= excelBook['Compas']\n","  data= Compas.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n","  10# Create a DataFrame based on the second and subsequent lines of data\n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover50.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Compas', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Accuracy 0.6596434359805511\n","Accuracy 0.6823338735818476\n","Accuracy 0.6682887266828873\n","Accuracy 0.6772100567721006\n","Accuracy 0.6553122465531225\n","Accuracy 0.6693679092382496\n","Accuracy 0.6620745542949756\n","Accuracy 0.6739659367396593\n","Accuracy 0.6593673965936739\n","Accuracy 0.67558799675588\n","Accuracy 0.673419773095624\n","Accuracy 0.6904376012965965\n","Accuracy 0.6545012165450121\n","Accuracy 0.6666666666666666\n","Accuracy 0.6723438767234388\n","Accuracy 0.6709886547811994\n","Accuracy 0.6636952998379254\n","Accuracy 0.667477696674777\n","Accuracy 0.6877534468775345\n","Accuracy 0.6618004866180048\n","Accuracy 0.673419773095624\n","Accuracy 0.640194489465154\n","Accuracy 0.6585563665855637\n","Accuracy 0.6723438767234388\n","Accuracy 0.6772100567721006\n","Accuracy 0.6726094003241491\n","Accuracy 0.6774716369529984\n","Accuracy 0.6642335766423357\n","Accuracy 0.683698296836983\n","Accuracy 0.6650446066504461\n","Accuracy 0.6823338735818476\n","Accuracy 0.6612641815235009\n","Accuracy 0.6496350364963503\n","Accuracy 0.681265206812652\n","Accuracy 0.6634225466342255\n","Accuracy 0.6596434359805511\n","Accuracy 0.6653160453808752\n","Accuracy 0.667477696674777\n","Accuracy 0.6966747769667477\n","Accuracy 0.6690997566909975\n","Accuracy 0.6766612641815235\n","Accuracy 0.6815235008103727\n","Accuracy 0.6820762368207623\n","Accuracy 0.6480129764801298\n","Accuracy 0.67558799675588\n","Accuracy 0.6628849270664505\n","Accuracy 0.6612641815235009\n","Accuracy 0.6845093268450932\n","Accuracy 0.656934306569343\n","Accuracy 0.6658556366585564\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SglOigoJKEiy"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"vP-IlZfPKFU6"},"source":["# PR REMOVER 75\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N4pHADeEKFU7","executionInfo":{"status":"ok","timestamp":1629690766103,"user_tz":-570,"elapsed":1640082,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"8f195d81-4d5c-4da6-9936-203e07b34ba3"},"source":["for i in range(1,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Compas/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Compas/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","\n","  # # normalization of train and test sets\n","  # Fitter= MM.fit(train)\n","  # transformed_train=Fitter.transform(train)\n","  # train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  # #test normalization\n","  # transformed_test=Fitter.transform(test)\n","  # test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","  #BLD Train set\n","  class Train(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Train, self).__init__(df=train  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","  # BLD Test\n","  BLD_Train = Train(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","  #******************************** prejudice remover regularizer*****************************\n","  Classifier = PrejudiceRemover(eta= 75, sensitive_attr= 'race')\n","  Classifier.fit(BLD_Train)\n","  prediction= Classifier.predict(BLD_Test)\n","  #**********************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS****************************\n","  #predicted labels\n","  # gbm_Predictions= best_model.predict(Test)\n","  # gbm_Predictions= gbm_Predictions.as_data_frame()\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= prediction.labels\n","\n","  # ********************COMPUTE DISCRIMINATION*****************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","\n","  excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover75.xlsx')\n","  Compas= excelBook['Compas']\n","  data= Compas.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n","  10# Create a DataFrame based on the second and subsequent lines of data\n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover75.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Compas', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Accuracy 0.6555915721231766\n","Accuracy 0.6815235008103727\n","Accuracy 0.6699107866991079\n","Accuracy 0.6731549067315491\n","Accuracy 0.6390916463909164\n","Accuracy 0.666936790923825\n","Accuracy 0.6620745542949756\n","Accuracy 0.656934306569343\n","Accuracy 0.6609894566098946\n","Accuracy 0.6731549067315491\n","Accuracy 0.6588330632090762\n","Accuracy 0.6782820097244733\n","Accuracy 0.6626115166261152\n","Accuracy 0.6528791565287916\n","Accuracy 0.6715328467153284\n","Accuracy 0.6685575364667747\n","Accuracy 0.6555915721231766\n","Accuracy 0.6690997566909975\n","Accuracy 0.683698296836983\n","Accuracy 0.6536901865369019\n","Accuracy 0.6685575364667747\n","Accuracy 0.6434359805510534\n","Accuracy 0.643146796431468\n","Accuracy 0.6723438767234388\n","Accuracy 0.67558799675588\n","Accuracy 0.6766612641815235\n","Accuracy 0.6701782820097245\n","Accuracy 0.6520681265206812\n","Accuracy 0.683698296836983\n","Accuracy 0.6609894566098946\n","Accuracy 0.6807131280388979\n","Accuracy 0.6636952998379254\n","Accuracy 0.6585563665855637\n","Accuracy 0.6820762368207623\n","Accuracy 0.6690997566909975\n","Accuracy 0.6636952998379254\n","Accuracy 0.660453808752026\n","Accuracy 0.6520681265206812\n","Accuracy 0.6658556366585564\n","Accuracy 0.6634225466342255\n","Accuracy 0.673419773095624\n","Accuracy 0.686385737439222\n","Accuracy 0.6723438767234388\n","Accuracy 0.6317923763179237\n","Accuracy 0.6690997566909975\n","Accuracy 0.6636952998379254\n","Accuracy 0.6726094003241491\n","Accuracy 0.67558799675588\n","Accuracy 0.6536901865369019\n","Accuracy 0.6707218167072182\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NDxf1UeRVgOp"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"eNlkgkHyKHkO"},"source":["# PR REMOVER 100\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"25xsmsZAKHkS","executionInfo":{"status":"ok","timestamp":1629692421272,"user_tz":-570,"elapsed":1655191,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"d48ef3f5-4e33-4822-e063-a6a06865934f"},"source":["for i in range(1,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Compas/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Compas/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","\n","  # # normalization of train and test sets\n","  # Fitter= MM.fit(train)\n","  # transformed_train=Fitter.transform(train)\n","  # train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  # #test normalization\n","  # transformed_test=Fitter.transform(test)\n","  # test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","  #BLD Train set\n","  class Train(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Train, self).__init__(df=train  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","  # BLD Test\n","  BLD_Train = Train(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  \n","  #******************************** prejudice remover regularizer*****************************\n","  Classifier = PrejudiceRemover(eta= 100, sensitive_attr= 'race')\n","  Classifier.fit(BLD_Train)\n","  prediction= Classifier.predict(BLD_Test)\n","  #**********************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS****************************\n","  #predicted labels\n","  # gbm_Predictions= best_model.predict(Test)\n","  # gbm_Predictions= gbm_Predictions.as_data_frame()\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= prediction.labels\n","\n","  # ********************COMPUTE DISCRIMINATION*****************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","\n","  excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover100.xlsx')\n","  Compas= excelBook['Compas']\n","  data= Compas.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n","  10# Create a DataFrame based on the second and subsequent lines of data\n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/PRemover/PRemover100.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Compas', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Accuracy 0.6612641815235009\n","Accuracy 0.6750405186385737\n","Accuracy 0.6447688564476886\n","Accuracy 0.67558799675588\n","Accuracy 0.635036496350365\n","Accuracy 0.6636952998379254\n","Accuracy 0.6636952998379254\n","Accuracy 0.656934306569343\n","Accuracy 0.6577453365774534\n","Accuracy 0.67558799675588\n","Accuracy 0.6515397082658023\n","Accuracy 0.6726094003241491\n","Accuracy 0.6528791565287916\n","Accuracy 0.6650446066504461\n","Accuracy 0.6593673965936739\n","Accuracy 0.6661264181523501\n","Accuracy 0.6442463533225283\n","Accuracy 0.6666666666666666\n","Accuracy 0.6593673965936739\n","Accuracy 0.6399026763990268\n","Accuracy 0.6628849270664505\n","Accuracy 0.640194489465154\n","Accuracy 0.656934306569343\n","Accuracy 0.681265206812652\n","Accuracy 0.6780210867802109\n","Accuracy 0.6742301458670988\n","Accuracy 0.6693679092382496\n","Accuracy 0.6390916463909164\n","Accuracy 0.6731549067315491\n","Accuracy 0.6658556366585564\n","Accuracy 0.6855753646677472\n","Accuracy 0.6539708265802269\n","Accuracy 0.6439578264395782\n","Accuracy 0.6577453365774534\n","Accuracy 0.6626115166261152\n","Accuracy 0.6612641815235009\n","Accuracy 0.6572123176661264\n","Accuracy 0.6463909164639091\n","Accuracy 0.6609894566098946\n","Accuracy 0.6682887266828873\n","Accuracy 0.6685575364667747\n","Accuracy 0.6782820097244733\n","Accuracy 0.6528791565287916\n","Accuracy 0.6269261962692619\n","Accuracy 0.6796431467964315\n","Accuracy 0.6499189627228525\n","Accuracy 0.6661264181523501\n","Accuracy 0.6747769667477697\n","Accuracy 0.6626115166261152\n","Accuracy 0.6618004866180048\n"],"name":"stdout"}]}]}