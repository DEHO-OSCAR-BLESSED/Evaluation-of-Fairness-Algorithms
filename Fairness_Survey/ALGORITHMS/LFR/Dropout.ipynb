{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Dropout.ipynb","provenance":[{"file_id":"1RyrIFd1EbO7wTWXg1693zq8SahcbsU8L","timestamp":1628685738191},{"file_id":"1zUVqlZwnp2gUWV_U6552PNihQarJC9I9","timestamp":1628683226587},{"file_id":"1AJWD--4mn4SFR1x2nX38Hkscv0C2Ovqi","timestamp":1628499839227}],"authorship_tag":"ABX9TyNM+o5x//U0CrovDk4eS/ar"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ff_Jv8Ptu4lP"},"source":["\n","# INSTALLATION"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q5qYRG8zufHw","executionInfo":{"status":"ok","timestamp":1630050870781,"user_tz":-570,"elapsed":13662,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"6d1c3be2-669e-4f05-e566-bd6439d70296"},"source":["!pip install aif360\n","!pip install fairlearn"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting aif360\n","  Downloading aif360-0.4.0-py3-none-any.whl (175 kB)\n","\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 61 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 92 kB 10.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 102 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 112 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 122 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 133 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 143 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 153 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 163 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 174 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 175 kB 8.0 MB/s \n","\u001b[?25hRequirement already satisfied: scipy<1.6.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.4.1)\n","Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from aif360) (0.22.2.post1)\n","Collecting tempeh\n","  Downloading tempeh-0.1.12-py3-none-any.whl (39 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from aif360) (3.2.2)\n","Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.19.5)\n","Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.1.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->aif360) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->aif360) (1.0.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (0.10.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360) (2.23.0)\n","Collecting memory-profiler\n","  Downloading memory_profiler-0.58.0.tar.gz (36 kB)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360) (3.6.4)\n","Collecting shap\n","  Downloading shap-0.39.0.tar.gz (356 kB)\n","\u001b[K     |████████████████████████████████| 356 kB 47.8 MB/s \n","\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler->tempeh->aif360) (5.4.8)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (1.10.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (21.2.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (8.8.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (1.4.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (0.7.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (57.4.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (2021.5.30)\n","Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360) (4.62.0)\n","Collecting slicer==0.0.7\n","  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n","Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360) (0.51.2)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360) (1.3.0)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap->tempeh->aif360) (0.34.0)\n","Building wheels for collected packages: memory-profiler, shap\n","  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for memory-profiler: filename=memory_profiler-0.58.0-py3-none-any.whl size=30190 sha256=640f7cdac8da345614264c5216b6f417796c579ec7560cb54b270c6b86c56cee\n","  Stored in directory: /root/.cache/pip/wheels/56/19/d5/8cad06661aec65a04a0d6785b1a5ad035cb645b1772a4a0882\n","  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for shap: filename=shap-0.39.0-cp37-cp37m-linux_x86_64.whl size=491648 sha256=416bf2c41b01d2961f1cddc399bc07563b3617be4fa9df9ec3763d10e0b71205\n","  Stored in directory: /root/.cache/pip/wheels/ca/25/8f/6ae5df62c32651cd719e972e738a8aaa4a87414c4d2b14c9c0\n","Successfully built memory-profiler shap\n","Installing collected packages: slicer, shap, memory-profiler, tempeh, aif360\n","Successfully installed aif360-0.4.0 memory-profiler-0.58.0 shap-0.39.0 slicer-0.0.7 tempeh-0.1.12\n","Collecting fairlearn\n","  Downloading fairlearn-0.7.0-py3-none-any.whl (177 kB)\n","\u001b[K     |████████████████████████████████| 177 kB 7.6 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (0.22.2.post1)\n","Requirement already satisfied: pandas>=0.25.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (1.1.5)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (1.19.5)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (1.4.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.1->fairlearn) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.1->fairlearn) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25.1->fairlearn) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->fairlearn) (1.0.1)\n","Installing collected packages: fairlearn\n","Successfully installed fairlearn-0.7.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TltW3iPkux0Q","executionInfo":{"status":"ok","timestamp":1630050871997,"user_tz":-570,"elapsed":1233,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"a1d49a89-dd6d-40c9-f8d7-cd2de3622175"},"source":["!apt-get install -jre\n","!java -version"],"execution_count":2,"outputs":[{"output_type":"stream","text":["E: Command line option 'j' [from -jre] is not understood in combination with the other options.\n","openjdk version \"11.0.11\" 2021-04-20\n","OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04)\n","OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KssrNl8GvDYU","executionInfo":{"status":"ok","timestamp":1630050902739,"user_tz":-570,"elapsed":30746,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"516bf8cc-5569-421b-d3a8-7028a27e8404"},"source":["!pip install h2o"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting h2o\n","  Downloading h2o-3.32.1.6.tar.gz (168.4 MB)\n","\u001b[K     |████████████████████████████████| 168.4 MB 45 kB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from h2o) (2.23.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from h2o) (0.8.9)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from h2o) (0.16.0)\n","Collecting colorama>=0.3.8\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (1.24.3)\n","Building wheels for collected packages: h2o\n","  Building wheel for h2o (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for h2o: filename=h2o-3.32.1.6-py2.py3-none-any.whl size=168439194 sha256=e8000de00f1ca2ce522ccaae91a98e0ca7515e01371b442efa220d46173ff19e\n","  Stored in directory: /root/.cache/pip/wheels/ee/0f/51/849ba221c4c1b11a04efb4a3427dc9cb1c4dcde218c6c98b13\n","Successfully built h2o\n","Installing collected packages: colorama, h2o\n","Successfully installed colorama-0.4.4 h2o-3.32.1.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_NQn2JJ0uw6u","executionInfo":{"status":"ok","timestamp":1630050906003,"user_tz":-570,"elapsed":3284,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"7d3d0429-c0e0-4c9b-8fee-070d4acd22d9"},"source":["!pip install xlsxwriter"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting xlsxwriter\n","  Downloading XlsxWriter-3.0.1-py3-none-any.whl (148 kB)\n","\u001b[?25l\r\u001b[K     |██▏                             | 10 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 20 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 30 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 40 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 51 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 61 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 71 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 81 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 92 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 102 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 112 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 122 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 133 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 143 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 148 kB 6.7 MB/s \n","\u001b[?25hInstalling collected packages: xlsxwriter\n","Successfully installed xlsxwriter-3.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0YklbHpAxd8","executionInfo":{"status":"ok","timestamp":1630050911144,"user_tz":-570,"elapsed":5160,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"a41bba09-6620-4320-8617-d3d1e1edf37e"},"source":["!pip install BlackBoxAuditing"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting BlackBoxAuditing\n","  Downloading BlackBoxAuditing-0.1.54.tar.gz (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 8.0 MB/s \n","\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (2.6.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (3.2.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (1.1.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (1.19.5)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (1.3.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->BlackBoxAuditing) (1.15.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->BlackBoxAuditing) (2018.9)\n","Building wheels for collected packages: BlackBoxAuditing\n","  Building wheel for BlackBoxAuditing (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for BlackBoxAuditing: filename=BlackBoxAuditing-0.1.54-py2.py3-none-any.whl size=1394771 sha256=3de09fd1b45dc96909e7a924bf596741f8c4ebeded400439da2bf5babaca7757\n","  Stored in directory: /root/.cache/pip/wheels/05/9f/ee/541a74be4cf5dad17430e64d3276370ea7b6a834a76cb4215a\n","Successfully built BlackBoxAuditing\n","Installing collected packages: BlackBoxAuditing\n","Successfully installed BlackBoxAuditing-0.1.54\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-Y_uQ6vdvN4a"},"source":["#IMPORTS"]},{"cell_type":"code","metadata":{"id":"rf1aISz6vGfR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630050921561,"user_tz":-570,"elapsed":10436,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"0676eade-8b73-42d3-d296-b2ad8d590d87"},"source":["import numpy as np\n","from mlxtend.feature_selection import  ExhaustiveFeatureSelector\n","from xgboost import  XGBClassifier\n","# import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import openpyxl\n","import xlsxwriter\n","from openpyxl import  load_workbook\n","\n","import BlackBoxAuditing\n","import shap\n","#suppress setwith copy warning\n","pd.set_option('mode.chained_assignment',None)\n","from sklearn.feature_selection import VarianceThreshold\n","from sklearn.feature_selection import SelectKBest, SelectFwe, SelectPercentile,SelectFdr, SelectFpr, SelectFromModel\n","from sklearn.feature_selection import chi2, mutual_info_classif\n","# from skfeature.function.similarity_based import fisher_score\n","import aif360\n","import matplotlib.pyplot as plt\n","from aif360.metrics.classification_metric import ClassificationMetric\n","\n","from aif360.metrics import BinaryLabelDatasetMetric\n","from aif360.algorithms.preprocessing import DisparateImpactRemover, Reweighing, LFR,OptimPreproc\n","from aif360.datasets import StandardDataset , BinaryLabelDataset\n","from sklearn.preprocessing import MinMaxScaler \n","MM= MinMaxScaler()\n","import h2o\n","from h2o.automl import H2OAutoML\n","from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n","\n","import sys\n","sys.path.append(\"../\")\n","import os\n","\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":546},"id":"RcxQeeX7vUXz","executionInfo":{"status":"ok","timestamp":1630050928861,"user_tz":-570,"elapsed":7351,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"07c6a55b-f09a-492d-e991-31e4c1f9d7ce"},"source":["h2o.init()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n","Attempting to start a local H2O server...\n","  Java Version: openjdk version \"11.0.11\" 2021-04-20; OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04); OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing)\n","  Starting server from /usr/local/lib/python3.7/dist-packages/h2o/backend/bin/h2o.jar\n","  Ice root: /tmp/tmp9d8e_j4o\n","  JVM stdout: /tmp/tmp9d8e_j4o/h2o_unknownUser_started_from_python.out\n","  JVM stderr: /tmp/tmp9d8e_j4o/h2o_unknownUser_started_from_python.err\n","  Server is running at http://127.0.0.1:54321\n","Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n","<td>03 secs</td></tr>\n","<tr><td>H2O_cluster_timezone:</td>\n","<td>Etc/UTC</td></tr>\n","<tr><td>H2O_data_parsing_timezone:</td>\n","<td>UTC</td></tr>\n","<tr><td>H2O_cluster_version:</td>\n","<td>3.32.1.6</td></tr>\n","<tr><td>H2O_cluster_version_age:</td>\n","<td>7 days, 13 hours and 5 minutes </td></tr>\n","<tr><td>H2O_cluster_name:</td>\n","<td>H2O_from_python_unknownUser_r5lyfi</td></tr>\n","<tr><td>H2O_cluster_total_nodes:</td>\n","<td>1</td></tr>\n","<tr><td>H2O_cluster_free_memory:</td>\n","<td>3.172 Gb</td></tr>\n","<tr><td>H2O_cluster_total_cores:</td>\n","<td>2</td></tr>\n","<tr><td>H2O_cluster_allowed_cores:</td>\n","<td>2</td></tr>\n","<tr><td>H2O_cluster_status:</td>\n","<td>accepting new members, healthy</td></tr>\n","<tr><td>H2O_connection_url:</td>\n","<td>http://127.0.0.1:54321</td></tr>\n","<tr><td>H2O_connection_proxy:</td>\n","<td>{\"http\": null, \"https\": null}</td></tr>\n","<tr><td>H2O_internal_security:</td>\n","<td>False</td></tr>\n","<tr><td>H2O_API_Extensions:</td>\n","<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n","<tr><td>Python_version:</td>\n","<td>3.7.11 final</td></tr></table></div>"],"text/plain":["--------------------------  ------------------------------------------------------------------\n","H2O_cluster_uptime:         03 secs\n","H2O_cluster_timezone:       Etc/UTC\n","H2O_data_parsing_timezone:  UTC\n","H2O_cluster_version:        3.32.1.6\n","H2O_cluster_version_age:    7 days, 13 hours and 5 minutes\n","H2O_cluster_name:           H2O_from_python_unknownUser_r5lyfi\n","H2O_cluster_total_nodes:    1\n","H2O_cluster_free_memory:    3.172 Gb\n","H2O_cluster_total_cores:    2\n","H2O_cluster_allowed_cores:  2\n","H2O_cluster_status:         accepting new members, healthy\n","H2O_connection_url:         http://127.0.0.1:54321\n","H2O_connection_proxy:       {\"http\": null, \"https\": null}\n","H2O_internal_security:      False\n","H2O_API_Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n","Python_version:             3.7.11 final\n","--------------------------  ------------------------------------------------------------------"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"RQVI-ISXvrZm"},"source":["#**************************LOADING DATASET*******************************"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FEGPULDrvk3g","executionInfo":{"status":"ok","timestamp":1630050976911,"user_tz":-570,"elapsed":40197,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"b27e975c-0eac-45f5-8020-f3e9e5b8f07e"},"source":["from google.colab import drive \n","drive.mount('/content/gdrive')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rA0wTXKH-csL","executionInfo":{"status":"aborted","timestamp":1630050933976,"user_tz":-570,"elapsed":8,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":["\n","# advantagedGroup= [{'HOME_LANGUAGE':1}]\n","# disadvantagedGroup= [{'HOME_LANGUAGE':0}]\n","# tr=pd.read_csv(r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Dropout/Test/Test1.csv')\n","# tr\n","# tester= BinaryLabelDataset(favorable_label=1,\n","#                                 unfavorable_label=0,\n","#                                 df=tr,\n","#                                 label_names=['PROGRAM_ENROLMENT_STATUS'],\n","#                                 protected_attribute_names=['HOME_LANGUAGE'],\n","#                                 unprivileged_protected_attributes=[[0]],\n","#                                 privileged_protected_attributes=[[1]])\n","# tester\n","# TR = LFR(unprivileged_groups=disadvantagedGroup,\n","#          privileged_groups=advantagedGroup,\n","#          k=10, Ax=0.1, Ay=1.0, Az=2.0,\n","#         #  verbose=1\n","#         )\n","# TR = TR.fit(tester, maxiter=5000, maxfun=5000)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YiMTx-YfV94X","executionInfo":{"status":"aborted","timestamp":1630050933977,"user_tz":-570,"elapsed":9,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":["# transformed = TR.transform(tester)\n","# transformed.labels= tester.labels\n","# transformed.protected_attributes= tester.protected_attributes\n","# transformed.feature_names\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qDh3f5HwHubq"},"source":["# GBM LFR\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"uN9VfZBAvxCj","executionInfo":{"status":"aborted","timestamp":1630050933978,"user_tz":-570,"elapsed":10,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":["for i in range(2,52,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/LFR/FairData/Dropout/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('PROGRAM_ENROLMENT_STATUS')\n","  train.insert(0, 'PROGRAM_ENROLMENT_STATUS', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/LFR/FairData/Dropout/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('PROGRAM_ENROLMENT_STATUS')\n","  test.insert(0, 'PROGRAM_ENROLMENT_STATUS', first_column)\n","\n","  #********************************************************binary labels for LFR*************************************************************\n","  # advantagedGroup= [{'HOME_LANGUAGE':1}]\n","  # disadvantagedGroup= [{'HOME_LANGUAGE':0}]\n","  # bldTrain= BinaryLabelDataset(favorable_label=1,\n","  #                               unfavorable_label=0,\n","  #                               df=train,\n","  #                               label_names=['PROGRAM_ENROLMENT_STATUS'],\n","  #                               protected_attribute_names=['HOME_LANGUAGE'],\n","  #                               unprivileged_protected_attributes=[[0]],\n","  #                               privileged_protected_attributes=[[1]])\n","  \n","  # bldTest=  BinaryLabelDataset(favorable_label=1,\n","  #                               unfavorable_label=0,\n","  #                               df=test,\n","  #                               label_names=['PROGRAM_ENROLMENT_STATUS'],\n","  #                               protected_attribute_names=['HOME_LANGUAGE'],\n","  #                               unprivileged_protected_attributes=[[0]],\n","  #                               privileged_protected_attributes=[[1]])\n","  # #*******************************************************LFR instance**************************************************************\n","  # TR = LFR(unprivileged_groups=disadvantagedGroup,\n","  #        privileged_groups=advantagedGroup,\n","  #        k=10, Ax=0.1, Ay=1.0, Az=2.0)\n","  # TR = TR.fit(bldTrain, maxiter=5000, maxfun=5000)\n","  \n","  # #setting the label and the protected groups of the transformed to the original so that only\n","  # #features are transformed. in order for LFR to be used as pre_processing algorithm\n","  \n","  # #transforming and setting transformed train labels and protected attributes\n","  # LFR_Train = TR .transform(bldTrain )\n","  # LFR_Train.labels= bldTrain.labels\n","  # # LFR_Train.protected_attributes= bldTrain.protected_attributes\n","\n","  # #transforming and setting transformed test labels and protected attributes\n","  # LFR_Test = TR .transform(bldTest)\n","  # LFR_Test.labels= bldTest.labels\n","  # # LFR_Test.protected_attributes= bldTest.protected_attributes\n"," \n","  \n","  # #*****************************************Repaired Train and Test Set NB: home language is at index 3*******************************************************\n","  # #using bldTest and bldTrain protected attr vals which is the old protected attributes as we are not using the transformed version of it created by the LFR\n","  # train=  pd.DataFrame(np.hstack([LFR_Train .labels, LFR_Train .features[:,0:3],bldTrain.protected_attributes,LFR_Train.features[:,4:]]),columns=train.columns)\n","  # test=  pd.DataFrame(np.hstack([LFR_Test .labels, LFR_Test .features[:,0:3],bldTest.protected_attributes,LFR_Test.features[:,4:]]),columns=test.columns)\n","  # # TotalRepairedDF= pd.concat([RepairedTrain ,RepairedTest ])\n","\n","\n","  # # normalization of train and test sets\n","  # Fitter= MM.fit(train)\n","  # transformed_train=Fitter.transform(train)\n","  # train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  # #test normalization\n","  # transformed_test=Fitter.transform(test)\n","  # test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","  # *************CHECKING FAIRNESS IN DATASET**************************\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","\n","\n","  #Transforming the Train and Test Set to BinaryLabel\n","  advantagedGroup= [{'HOME_LANGUAGE':1}]\n","  disadvantagedGroup= [{'HOME_LANGUAGE':0}]\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'PROGRAM_ENROLMENT_STATUS',\n","                  favorable_classes= [1],protected_attribute_names=['HOME_LANGUAGE'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['HOME_LANGUAGE'],\n","                        privileged_classes= [[1]])\n","  ## ********************Checking Bias Repaired Data********************************\n","  DataBias_Checker = BinaryLabelDatasetMetric(BLD_Test, unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","\n","  dsp= DataBias_Checker .statistical_parity_difference() \n","  dif= DataBias_Checker.consistency()  \n","  ddi= DataBias_Checker.disparate_impact() \n","\n","  print('The Statistical Parity diference is = {diff}'.format(diff=  dsp  ))\n","  print('Individual Fairness is = {IF}'.format( IF=  dif ))\n","  print('Disparate Impact is = {IF}'.format( IF=   ddi  ))\n","  # ********************SETTING TO H20 FRAME AND MODEL TRAINING*******************************\n","  x = list(train.columns)\n","  y = \"PROGRAM_ENROLMENT_STATUS\"\n","  x.remove(y)\n","  Train=h2o.H2OFrame(train)\n","  Test= h2o.H2OFrame(test)\n","  Train[y] = Train[y].asfactor()\n","  Test[y] = Test[y].asfactor()\n","  aml = H2OAutoML(max_models=10, nfolds=10, include_algos=['GBM'] , stopping_metric='AUTO') #verbosity='info',,'GBM', 'DRF'\n","  aml.train(x=x, y=y, training_frame=Train)\n","  best_model= aml.leader\n","  # a.model_performance()\n","  #**********************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS****************************\n","  #predicted labels\n","  gbm_Predictions= best_model.predict(Test)\n","  gbm_Predictions= gbm_Predictions.as_data_frame()\n","  predicted_df= test.copy()\n","  predicted_df['PROGRAM_ENROLMENT_STATUS']= gbm_Predictions.predict.to_numpy()\n","\n","  # ********************COMPUTE DISCRIMINATION*****************************\n","\n","  advantagedGroup= [{'HOME_LANGUAGE':1}]\n","  disadvantagedGroup= [{'HOME_LANGUAGE':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'PROGRAM_ENROLMENT_STATUS',\n","                  favorable_classes= [1],protected_attribute_names=['HOME_LANGUAGE'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['HOME_LANGUAGE'],\n","                        privileged_classes= [[1]])\n","\n","  # # Workbook= pd.ExcelFile(r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/BaseLines/GBM/gbm_Results.xlsx')\n","  # excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/BaseLines/GBM/gbm_Results.xlsx')\n","  # OldDF= excelBook.get_sheet_by_name(\"Dropout\")#pd.read_excel(Workbook,sheet_name='Dropout')\n","  #load workbook\n","  excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/LFR/NewLFR_Results/gbm_Results.xlsx')\n","  Dropout= excelBook['Dropout']\n","  data= Dropout.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n","  10# Create a DataFrame based on the second and subsequent lines of data\n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF,\n","                                          'DATA_SP':dsp,'DATA_CONS':dif,'DATA_DI':ddi})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/LFR/NewLFR_Results/gbm_Results.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Dropout', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4H3vvxWBboi5"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"Hoc62jY7Olbt"},"source":["#LOGISTIC REGRESSION LFR\n"]},{"cell_type":"code","metadata":{"id":"RZmY0q8iVY3O","executionInfo":{"status":"aborted","timestamp":1630050933980,"user_tz":-570,"elapsed":12,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":["for i in range(2,52,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/LFR/FairData/Dropout/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('PROGRAM_ENROLMENT_STATUS')\n","  train.insert(0, 'PROGRAM_ENROLMENT_STATUS', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/LFR/FairData/Dropout/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('PROGRAM_ENROLMENT_STATUS')\n","  test.insert(0, 'PROGRAM_ENROLMENT_STATUS', first_column)\n","\n","  # #********************************************************binary labels for LFR*************************************************************\n","\n","  # bldTrain= BinaryLabelDataset(favorable_label=1,\n","  #                               unfavorable_label=0,\n","  #                               df=train,\n","  #                               label_names=['PROGRAM_ENROLMENT_STATUS'],\n","  #                               protected_attribute_names=['HOME_LANGUAGE'],\n","  #                               unprivileged_protected_attributes=[[0]],\n","  #                               privileged_protected_attributes=[[1]])\n","  \n","  # bldTest=  BinaryLabelDataset(favorable_label=1,\n","  #                               unfavorable_label=0,\n","  #                               df=test,\n","  #                               label_names=['PROGRAM_ENROLMENT_STATUS'],\n","  #                               protected_attribute_names=['HOME_LANGUAGE'],\n","  #                               unprivileged_protected_attributes=[[0]],\n","  #                               privileged_protected_attributes=[[1]])\n","  #  #*******************************************************LFR instance**************************************************************\n","  # TR = LFR(unprivileged_groups=disadvantagedGroup,\n","  #        privileged_groups=advantagedGroup,\n","  #        k=10, Ax=0.1, Ay=1.0, Az=2.0,\n","  #       #  verbose=1\n","  #       )\n","  # TR = TR.fit(bldTrain, maxiter=5000, maxfun=5000)\n","  \n","  # #setting the label and the protected groups of the transformed to the original so that only\n","  # #features are transformed. in order for LFR to be used as pre_processing algorithm\n","  \n","  # #transforming and setting transformed train labels and protected attributes\n","  # LFR_Train = TR .transform(bldTrain )\n","  # LFR_Train.labels= bldTrain.labels\n","  # # LFR_Train.protected_attributes= bldTrain.protected_attributes\n","\n","  # #transforming and setting transformed test labels and protected attributes\n","  # LFR_Test = TR .transform(bldTest)\n","  # LFR_Test.labels= bldTest.labels\n","  # # LFR_Test.protected_attributes= bldTest.protected_attributes\n"," \n","  # #*****************************************Repaired Train and Test Set NB: home language is at index 3*******************************************************\n","  # #using bldTest and bldTrain protected attr vals which is the old protected attributes as we are not using the transformed version of it created by the LFR\n","  # train=  pd.DataFrame(np.hstack([LFR_Train .labels, LFR_Train .features[:,0:3],bldTrain.protected_attributes,LFR_Train.features[:,4:]]),columns=train.columns)\n","  # test=  pd.DataFrame(np.hstack([LFR_Test .labels, LFR_Test .features[:,0:3],bldTest.protected_attributes,LFR_Test.features[:,4:]]),columns=test.columns)\n","\n","  # normalization of train and test sets\n","  Fitter= MM.fit(train)\n","  transformed_train=Fitter.transform(train)\n","  train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  #test normalization\n","  transformed_test=Fitter.transform(test)\n","  test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","  # *************CHECKING FAIRNESS IN DATASET**************************\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","\n","\n","  #Transforming the Train and Test Set to BinaryLabel\n","  advantagedGroup= [{'HOME_LANGUAGE':1}]\n","  disadvantagedGroup= [{'HOME_LANGUAGE':0}]\n","\n","  # class Train(StandardDataset):\n","  #     def __init__(self,label_name= 'PROGRAM_ENROLMENT_STATUS',\n","  #                  favorable_classes= [1],protected_attribute_names=['HOME_LANGUAGE'],   privileged_classes=[[1]], ):\n","\n","\n","  #         super(Train, self).__init__(df=train  , label_name=label_name ,\n","  #             favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","  #             privileged_classes=privileged_classes ,\n","  #            )\n","\n","\n","\n","\n","\n","  # BLD_Train= Train(protected_attribute_names= ['HOME_LANGUAGE'],\n","  #                        privileged_classes= [[1]])\n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'PROGRAM_ENROLMENT_STATUS',\n","                  favorable_classes= [1],protected_attribute_names=['HOME_LANGUAGE'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['HOME_LANGUAGE'],\n","                        privileged_classes= [[1]])\n","  ## ********************Checking Bias in Data********************************\n","  DataBias_Checker = BinaryLabelDatasetMetric(BLD_Test    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","\n","  dsp= DataBias_Checker .statistical_parity_difference() \n","  dif= DataBias_Checker.consistency()  \n","  ddi= DataBias_Checker.disparate_impact() \n","\n","  print('The Statistical Parity diference is = {diff}'.format(diff=  dsp  ))\n","  print('Individual Fairness is = {IF}'.format( IF=  dif ))\n","  print('Disparate Impact is = {IF}'.format( IF=   ddi  ))\n","  # ********************SETTING TO H20 FRAME AND MODEL TRAINING*******************************\n","  x = list(train.columns)\n","  y = \"PROGRAM_ENROLMENT_STATUS\"\n","  x.remove(y)\n","  Train=h2o.H2OFrame(train)\n","  Test= h2o.H2OFrame(test)\n","  Train[y] = Train[y].asfactor()\n","  Test[y] = Test[y].asfactor()\n","  LogReg = H2OGeneralizedLinearEstimator(family= \"binomial\", lambda_ = 0)\n","  LogReg.train(x=x, y=y, training_frame=Train)\n","\n","  LogReg_Predictions= LogReg.predict(Test)\n","  LogReg_Predictions= LogReg_Predictions.as_data_frame()\n","  # *************************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS**************************************\n","  predicted_df= test.copy()\n","  predicted_df['PROGRAM_ENROLMENT_STATUS']= LogReg_Predictions.predict.to_numpy()\n","\n","  # ***************************COMPUTE DISCRIMINATION********************************\n","\n","  advantagedGroup= [{'HOME_LANGUAGE':1}]\n","  disadvantagedGroup= [{'HOME_LANGUAGE':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'PROGRAM_ENROLMENT_STATUS',\n","                  favorable_classes= [1],protected_attribute_names=['HOME_LANGUAGE'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['HOME_LANGUAGE'],\n","                        privileged_classes= [[1]])\n","\n","\n","  excelBook= load_workbook(r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/LFR/NewLFR_Results/LR_Results.xlsx')\n","  Dropout= excelBook['Dropout']\n","  data= Dropout.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n"," \n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF,\n","                                          'DATA_SP':dsp,'DATA_CONS':dif,'DATA_DI':ddi})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/LFR/NewLFR_Results/LR_Results.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Dropout', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8F81NoFWp2bQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630065278432,"user_tz":-570,"elapsed":13431048,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"bbd4572a-1b2c-44a2-bd90-e02af0df0e3c"},"source":["for i in range(30,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/LFR/FairData/Law/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)#.drop('region_first',axis=1)\n","  first_column = train.pop('first_pf')\n","  train.insert(0, 'first_pf', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/LFR/FairData/Law/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)#.drop('region_first',axis=1)\n","  first_column = test.pop('first_pf')\n","  test.insert(0, 'first_pf', first_column)\n","\n","  #********************************************************binary labels for LFR*************************************************************\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","  # bldTrain= BinaryLabelDataset(favorable_label=1,\n","  #                               unfavorable_label=0,\n","  #                               df=train,\n","  #                               label_names=['first_pf'],\n","  #                               protected_attribute_names=['race'],\n","  #                               unprivileged_protected_attributes=[[0]],\n","  #                               privileged_protected_attributes=[[1]])\n","  \n","  # bldTest=  BinaryLabelDataset(favorable_label=1,\n","  #                               unfavorable_label=0,\n","  #                               df=test,\n","  #                               label_names=['first_pf'],\n","  #                               protected_attribute_names=['race'],\n","  #                               unprivileged_protected_attributes=[[0]],\n","  #                               privileged_protected_attributes=[[1]])\n","  # #*******************************************************LFR instance**************************************************************\n","  # TR = LFR(unprivileged_groups=disadvantagedGroup,\n","  #        privileged_groups=advantagedGroup)\n","  # TR = TR.fit(bldTrain, maxiter=5000, maxfun=5000)\n","  \n","  # #  ********************NB: the follwoing comment on setting labels  may not be needed (apparently it may):\n","  # #setting the label and the protected groups of the transformed to the original so that only\n","  # #features are transformed. in order for LFR to be used as pre_processing algorithm (ref AIF30 slack convo with Hoffman)\n","  \n","  # #transforming and setting transformed train labels and protected attributes\n","  # LFR_Train = TR .transform(bldTrain )\n","  # LFR_Train.labels= bldTrain.labels\n","  # # LFR_Train.protected_attributes= bldTrain.protected_attributes\n","\n","  # #transforming and setting transformed test labels and protected attributes\n","  # LFR_Test = TR .transform(bldTest)\n","  # LFR_Test.labels= bldTest.labels\n","  # # LFR_Test.protected_attributes= bldTest.protected_attributes\n"," \n","  \n","  # #*****************************************Repaired Train and Test Set NB: home language is at index 3*******************************************************\n","  # #using bldTest and bldTrain protected attr vals which is the old protected attributes as we are not using the transformed version of it created by the LFR\n","  # train=  pd.DataFrame(np.hstack([LFR_Train .labels,bldTrain.protected_attributes,LFR_Train .features[:,1:]]),columns=train.columns) #[:,0:5],bldTrain.protected_attributes,LFR_Train.features[:,6:]\n","  # test=  pd.DataFrame(np.hstack([LFR_Test .labels,bldTest.protected_attributes, LFR_Test .features[:,1:]]),columns=test.columns)\n","\n"," \n","  # **********************binarizing the protected attribute and labels column this don't work************************\n","  # train ['race']= (train['race'] >= train['race'].median()).astype(int)\n","  # train['first_pf']= (train['first_pf'] >= train['first_pf'].median()).astype(int)\n","  \n","  #test\n","  # test ['race']= (test['race'] >= test['race'].median()).astype(int)\n","  # test['first_pf']= (test['first_pf'] >= test['first_pf'].median()).astype(int)\n","\n","\n","  # # # normalization of train and test sets\n","  # Fitter= MM.fit(train)\n","  # transformed_train=Fitter.transform(train)\n","  # train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  # #test normalization\n","  # transformed_test=Fitter.transform(test)\n","  # test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","  # *************CHECKING FAIRNESS IN DATASET**************************\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","\n","\n","  #Transforming the Train and Test Set to BinaryLabel\n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'first_pf',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  ## ********************Checking Bias Repaired Data********************************\n","  DataBias_Checker = BinaryLabelDatasetMetric(BLD_Test    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","\n","  dsp= DataBias_Checker .statistical_parity_difference() \n","  dif= DataBias_Checker.consistency()  \n","  ddi= DataBias_Checker.disparate_impact() \n","\n","  print('The Statistical Parity diference is = {diff}'.format(diff=  dsp  ))\n","  print('Individual Fairness is = {IF}'.format( IF=  dif ))\n","  print('Disparate Impact is = {IF}'.format( IF=   ddi  ))\n","  # ********************SETTING TO H20 FRAME AND MODEL TRAINING*******************************\n","  x = list(train.columns)\n","  y = \"first_pf\"\n","  x.remove(y)\n","  Train=h2o.H2OFrame(train)\n","  Test= h2o.H2OFrame(test)\n","  Train[y] = Train[y].asfactor()\n","  Test[y] = Test[y].asfactor()\n","  aml = H2OAutoML(max_models=10, nfolds=10, include_algos=['GBM'] , stopping_metric='AUTO') #verbosity='info',,'GBM', 'DRF'\n","  aml.train(x=x, y=y, training_frame=Train)\n","  best_model= aml.leader\n","  # a.model_performance()\n","  #**********************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS****************************\n","  #predicted labels\n","  gbm_Predictions= best_model.predict(Test)\n","  gbm_Predictions= gbm_Predictions.as_data_frame()\n","  predicted_df= test.copy()\n","  predicted_df['first_pf']= gbm_Predictions.predict.to_numpy()\n","\n","  # ********************COMPUTE DISCRIMINATION*****************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'first_pf',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","  # # Workbook= pd.ExcelFile(r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/BaseLines/GBM/gbm_Results.xlsx')\n","  # excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/BaseLines/GBM/gbm_Results.xlsx')\n","  # OldDF= excelBook.get_sheet_by_name(\"Law\")#pd.read_excel(Workbook,sheet_name='Law')\n","  #load workbook\n","  excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/LFR/NewLFR_Results/gbm_Results.xlsx')\n","  Law= excelBook['Law']\n","  data= Law.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n","  10# Create a DataFrame based on the second and subsequent lines of data\n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF,\n","                                          'DATA_SP':dsp,'DATA_CONS':dif,'DATA_DI':ddi})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/LFR/NewLFR_Results/gbm_Results.xlsx'\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Law', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["The Statistical Parity diference is = 0.0172733387956584\n","Individual Fairness is = [0.96548876]\n","Disparate Impact is = 1.0336292291937452\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.49105094079853145\n","The Statistical Parity diference is = -0.19670754910061172\n","Individual Fairness is = [0.96164258]\n","Disparate Impact is = 0.6602184444832402\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.5170910759348475\n","The Statistical Parity diference is = -0.03530551036093327\n","Individual Fairness is = [0.97659477]\n","Disparate Impact is = 0.9337126873596427\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.38664525011473155\n","The Statistical Parity diference is = 0.1753146544495321\n","Individual Fairness is = [0.98141349]\n","Disparate Impact is = 1.5952484387532428\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.3088572739788894\n","The Statistical Parity diference is = -0.6629738211534522\n","Individual Fairness is = [0.97870583]\n","Disparate Impact is = 0.11787841940374862\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.39238182652592934\n","The Statistical Parity diference is = 0.05573831827652159\n","Individual Fairness is = [0.97150069]\n","Disparate Impact is = 1.1236976831357144\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.6027994492886646\n","The Statistical Parity diference is = -0.03766613269267749\n","Individual Fairness is = [0.98628126]\n","Disparate Impact is = 0.9132089503803663\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.5744436797430603\n","The Statistical Parity diference is = -0.19542188140157746\n","Individual Fairness is = [0.97957779]\n","Disparate Impact is = 0.6814963583212745\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.7530977512620468\n","The Statistical Parity diference is = 0.33512276509944233\n","Individual Fairness is = [0.97806333]\n","Disparate Impact is = 2.246022297396486\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.21385956860945388\n","The Statistical Parity diference is = -0.18423440971430705\n","Individual Fairness is = [0.97742084]\n","Disparate Impact is = 0.6477308488915887\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.6360715924736118\n","The Statistical Parity diference is = -0.2867196609756522\n","Individual Fairness is = [0.97820101]\n","Disparate Impact is = 0.589417445482866\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.6204681046351538\n","The Statistical Parity diference is = 0.2361803525293284\n","Individual Fairness is = [0.97375545]\n","Disparate Impact is = 1.548397185459286\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.6366139022711631\n","The Statistical Parity diference is = 0.00019299337143618978\n","Individual Fairness is = [0.98044975]\n","Disparate Impact is = 1.0005001613511784\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.5195043597980725\n","The Statistical Parity diference is = -0.13042417205273382\n","Individual Fairness is = [0.9738871]\n","Disparate Impact is = 0.7668691655301935\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.7778797613584213\n","The Statistical Parity diference is = -0.17876206394565158\n","Individual Fairness is = [0.97696191]\n","Disparate Impact is = 0.6441069128381279\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.6532813217072051\n","The Statistical Parity diference is = -0.25121324458228556\n","Individual Fairness is = [0.98031207]\n","Disparate Impact is = 0.6017588487048264\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.36071592473611747\n","The Statistical Parity diference is = 0.4442929857514759\n","Individual Fairness is = [0.98274834]\n","Disparate Impact is = 2.8568008284359037\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.4640972700160587\n","The Statistical Parity diference is = 0.11248206660252846\n","Individual Fairness is = [0.9757687]\n","Disparate Impact is = 1.2117786790496659\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.5456631482331344\n","The Statistical Parity diference is = 0.45532037766344285\n","Individual Fairness is = [0.95915558]\n","Disparate Impact is = 2.9247232275210004\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.5346489215236347\n","The Statistical Parity diference is = -0.4373693913571777\n","Individual Fairness is = [0.96902249]\n","Disparate Impact is = 0.34432623417410935\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.6645250114731528\n","The Statistical Parity diference is = -0.21432479920124642\n","Individual Fairness is = [0.96177145]\n","Disparate Impact is = 0.637963036598779\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","Parse progress: |█████████████████████████████████████████████████████████| 100%\n","AutoML progress: |████████████████████████████████████████████████████████| 100%\n","gbm prediction progress: |████████████████████████████████████████████████| 100%\n","Accuracy 0.42129417163836624\n"],"name":"stdout"}]}]}