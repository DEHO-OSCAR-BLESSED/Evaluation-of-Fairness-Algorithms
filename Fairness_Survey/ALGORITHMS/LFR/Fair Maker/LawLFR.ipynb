{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LawLFR.ipynb","provenance":[{"file_id":"1oO4CBSy33LzVueyRwn2M9Gg9IEBLtynJ","timestamp":1629977135032},{"file_id":"1duszuHgih-0NRgJfKHOsNdED8tRqDOQj","timestamp":1629977021931},{"file_id":"1n7keCA-XHVd5zpdpU6s-JPbbpo90lLef","timestamp":1629975168803},{"file_id":"1x89rMhL1bBUw9G19novjB4gVVsAIn19K","timestamp":1629974610153}],"authorship_tag":"ABX9TyO9+z1hCmw5cQMRX8dOMc5/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"fzCydXdn7G3B","executionInfo":{"status":"ok","timestamp":1630043190440,"user_tz":-570,"elapsed":468,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":["import sys\n","sys.path.append(\"../\")\n","import os\n","import numpy as np\n","import pandas as pd\n","import scipy.optimize as optim\n","from sklearn.preprocessing import StandardScaler\n"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d0NjeOPx75zO"},"source":["# HELPER FUNCTIONS"]},{"cell_type":"code","metadata":{"id":"G0Yk1odW7956","executionInfo":{"status":"ok","timestamp":1630043193386,"user_tz":-570,"elapsed":586,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":["import numpy as np\n","from scipy.special import softmax\n","import pandas as pd\n","\n","np.random.seed(509)\n","\n","\n","def loss_x(x_new, x_initial):\n","    \"\"\"\n","    Constrains the mapping to Z to be good description of X.\n","    Prototpyes should retain as much initial info as possible.\n","\n","    difference is measured by squared sum of difference\n","\n","\n","    ARGS:\n","    x_new - Prototypes\n","    x_initial - raw data\n","    \"\"\"\n","    return np.mean(np.sum(np.square((x_new - x_initial))))\n","\n","\n","def loss_y(y_true, y_predicted):\n","    \"\"\"\n","    This loss term requires that the prediction of y is as accurate as possible:\n","\n","    Computes log loss\n","\n","    ARGS:\n","    y_true - (num_examples, )\n","    y_predicted - (num_examples, )\n","    \"\"\"\n","    # logarithm is undefined in 0 which means y cant be 0 or 1 => we clip it\n","    y_true = np.clip(y_true, 1e-6, 0.999)\n","    y_predicted = np.clip(y_predicted, 1e-6, 0.999)\n","\n","    log_loss = np.sum(y_true * np.log(y_predicted) +\n","                      (1. - y_true) * np.log(1. - y_predicted)) / len(y_true)\n","\n","    return -log_loss\n","\n","\n","def loss_z(M_k_sensitive, M_k_non_sensitive):\n","    \"\"\"\n","    Ensures statistical parity\n","\n","    Calculates L1 distance\n","\n","    Args:\n","    M_k_sensitive - (num_prototypes, )\n","    M_k_non_sensitive - (num_prototypes, )\n","    \"\"\"\n","    return np.sum(np.abs(M_k_sensitive - M_k_non_sensitive))\n","\n","\n","def distances(X, v, alpha):\n","    \"\"\"\n","    Calculates distance between initial data and each of the prototypes \n","    Formula -> euclidean(x, v * alpha) (alpha is weight for each feature)\n","\n","    ARGS:\n","    X - (num_examples, num_features)\n","    v - (num_prototypes, num_features)\n","    alpha - (num_features, 1)\n","\n","    returns:\n","    dists - (num_examples, num_prototypes)\n","    \"\"\"\n","    num_examples = X.shape[0]\n","    num_prototypes = v.shape[0]\n","    dists = np.zeros(shape=(num_examples, num_prototypes))\n","\n","    # X = X.values  # converting to NumPy, this is needed in case you pass dataframe\n","    for i in range(num_examples):\n","        dist = np.square(X[i] - v)  # squarred distance\n","        dist_alpha = np.multiply(dist, alpha)  # multiplying by weights\n","        sum_ = np.sum(dist_alpha, axis=1)\n","        dists[i] = sum_\n","\n","    return dists\n","\n","\n","def M_nk(dists):\n","    \"\"\"\n","    define Mn,k as the probability that x maps to v\n","\n","    Given the definitions of the prototypes as points in\n","    the input space, a set of prototypes induces a natural\n","    probabilistic mapping from X to Z via the softmax\n","\n","    Since we already have distances calcutated we just map them to probabilities\n","\n","    NOTE:\n","    minus distance because smaller the distance better the mapping\n","\n","    ARGS:\n","    dists - (num_examples, num_prototypes)\n","\n","    Return :\n","    mappings - (num_examples, num_prototypes)\n","    \"\"\"\n","    return softmax(-dists, axis=1)  # specifying axis is important\n","\n","\n","def M_k(M_nk):\n","    \"\"\"\n","    Calculate mean of the mapping for each prototype\n","\n","    ARGS:\n","    M_nk - (num_examples, num_prototypes)\n","\n","    Returns:\n","    M_k - mean of the mappings (num_prototypes, )\n","    \"\"\"\n","    return np.mean(M_nk, axis=0)\n","\n","\n","def x_n_hat(M_nk, v):\n","    \"\"\"\n","    Gets new representation of the data, \n","    Performs simple dot product\n","\n","    ARGS:\n","    M_nk - (num_examples, num_prototypes)\n","    v - (num_prototypes, num_features)\n","\n","    Returns:\n","    x_n_hat - (num_examples, num_features)\n","    \"\"\"\n","    return M_nk @ v\n","\n","\n","def y_hat(M_nk, w):\n","    \"\"\"\n","    Function calculates labels in the new representation space\n","    Performs simple dot product\n","\n","    ARGS:\n","    M_nk - (num_examples, num_prototypes)\n","    w - (num_prototypes, )\n","\n","    returns:\n","    y_hat - (num_examples, )\n","    \"\"\"\n","    return M_nk @ w\n","\n","\n","def optim_objective(params, data_sensitive, data_non_sensitive, y_sensitive,\n","                    y_non_sensitive,  inference=False, NUM_PROTOTYPES=10, A_x=0.01, A_y=0.1, A_z=0.5,\n","                    print_every=100):\n","    \"\"\"\n","    Function gathers all the helper functions to calculate overall loss\n","\n","    This is further passed to l-bfgs optimizer \n","\n","    ARGS:\n","    params - vector of length (2 * num_features + NUM_PROTOTYPES + NUM_PROTOTYPES * num_features)\n","    data_sensitive - instances belonging to senstive group (num_sensitive_examples, num_features)\n","    data_non_sensitive - similar to data_sensitive (num_non_senitive_examplesm num_features)\n","    y_sensitive - labels for sensitive group (num_sensitive_examples, )\n","    y_non_sensitive - similar to y_sensitive\n","    inference - (optional) if True than will return new dataset instead of loss\n","    NUM_PROTOTYPES - (optional), first_pf 10\n","    A_x - (optional) hyperparameters for loss_X, first_pf 0.01\n","    A_y - (optional) hyperparameters for loss_Y, first_pf 1\n","    A_z - (optional) hyperparameters for loss_Z, first_pf 0.5\n","    print_every - (optional) how often to print loss, first_pf 100\n","    returns:\n","    if inference - False :\n","    float - A_x * L_x + A_y * L_y + A_z * L_z \n","    if inference - True:\n","    x_hat_sensitive, x_hat_non_sensitive, y_hat_sensitive, y_hat_non_sensitive\n","    \"\"\"\n","    optim_objective.iters += 1\n","\n","    num_features = data_sensitive.shape[1]\n","    # extract values for each variable from params vector\n","    alpha_non_sensitive = params[:num_features]\n","    alpha_sensitive = params[num_features:2 * num_features]\n","    w = params[2 * num_features:2 * num_features + NUM_PROTOTYPES]\n","    v = params[2 * num_features + NUM_PROTOTYPES:].reshape(NUM_PROTOTYPES, num_features)\n","\n","    dists_sensitive = distances(data_sensitive, v, alpha_sensitive)\n","    dists_non_sensitive = distances(data_non_sensitive, v, alpha_non_sensitive)\n","\n","    # get probabilities of mappings\n","    M_nk_sensitive = M_nk(dists_sensitive)\n","    M_nk_non_sensitive = M_nk(dists_non_sensitive)\n","\n","    # M_k only used for calcilating loss_y(statistical parity)\n","    M_k_sensitive = M_k(M_nk_sensitive)\n","    M_k_non_sensitive = M_k(M_nk_non_sensitive)\n","    L_z = loss_z(M_k_sensitive, M_k_non_sensitive)  # stat parity\n","\n","    # get new representation of data\n","    x_hat_sensitive = x_n_hat(M_nk_sensitive, v)\n","    x_hat_non_sensitive = x_n_hat(M_nk_non_sensitive, v)\n","    # calculates how close new representation is to original data\n","    L_x_sensitive = loss_x(data_sensitive, x_hat_sensitive)\n","    L_x_non_sensitive = loss_x(data_non_sensitive, x_hat_non_sensitive)\n","\n","    # get new values for labels\n","    y_hat_sensitive = y_hat(M_nk_sensitive, w)\n","    y_hat_non_sensitive = y_hat(M_nk_non_sensitive, w)\n","    # ensure how good new predictions are(log_loss)\n","    L_y_sensitive = loss_y(y_sensitive, y_hat_sensitive)\n","    L_y_non_sensitive = loss_y(y_non_sensitive, y_hat_non_sensitive)\n","\n","    L_x = L_x_sensitive + L_x_non_sensitive\n","    L_y = L_y_sensitive + L_y_non_sensitive\n","\n","    loss = A_x * L_x + A_y * L_y + A_z * L_z\n","\n","    if optim_objective.iters % print_every == 0:\n","        print(f'loss on iteration {optim_objective.iters} : {loss}, L_x - {L_x * A_x} L_y - {L_y * A_y} L_z - {L_z * A_z}')\n","    \n","    if not inference:\n","        return loss\n","    if inference:\n","        return x_hat_sensitive, x_hat_non_sensitive, y_hat_sensitive, y_hat_non_sensitive\n","\n","optim_objective.iters = 0\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"xO2k-oJ49Oes","executionInfo":{"status":"ok","timestamp":1630043102096,"user_tz":-570,"elapsed":12,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":[""],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"HYX_khar9SAO","executionInfo":{"status":"ok","timestamp":1630043102113,"user_tz":-570,"elapsed":28,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":[""],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h1CCt7Tq8RQ-"},"source":["# REPAIRER"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1u3DTYPw9X6w","executionInfo":{"status":"ok","timestamp":1630043145999,"user_tz":-570,"elapsed":43913,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"066d6e3a-90dd-4c17-937c-db0c278f1bdc"},"source":["from google.colab import drive \n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Gakq0-Z78PYe","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"error","timestamp":1630043154517,"user_tz":-570,"elapsed":8550,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"957656fc-9e52-498a-a57a-6494c0c608af"},"source":["# seperation into sensitive and non sensitive\n","\n","for count in range(7,51,1):\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Law/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(count)+ \".csv\"))\n","  data= pd.read_csv(train_path).drop('region_first', axis=1)\n","  first_column = data.pop('first_pf')\n","  data.insert(0, 'first_pf', first_column)\n","\n","\n","  data_sensitive = data.loc[data.race==0]\n","  data_non_sensitive = data[data.race ==1]\n","  y_sensitive = data_sensitive.first_pf\n","  y_non_sensitive = data_non_sensitive.first_pf\n","\n","  print (f'Dataset contains {data.shape[0]} examples and {data.shape[1]} features')\n","  print (f'From which {data_sensitive.shape[0]} belong to sensitive group and {data_non_sensitive.shape[0]} to non nensitive group ')\n","\n","  del data_sensitive['first_pf']\n","  del data_non_sensitive['first_pf']\n","\n","  # Standard Scaling\n","  data_sensitive = StandardScaler().fit_transform(data_sensitive)\n","  data_non_sensitive = StandardScaler().fit_transform(data_non_sensitive)\n","\n","\n","  NUM_PROTOTYPES = 10\n","  num_features = data_sensitive.shape[1]\n","\n","  params = np.random.uniform(size=(num_features * 2 + NUM_PROTOTYPES + NUM_PROTOTYPES * num_features))\n","  # here we generate random weight for each of the features both for sensitive data\n","  # and for non sensitive, hence num_features*2(in paper this is denoted as alpha)\n","  # alphas are used for calculating distances\n","\n","  # Then NUM_PROTOTYPES is a weight for each prototype, this is multiplied with \n","  # M_nk s and used for calculating y_hat\n","\n","  # Next is NUM_PROTOTYPES * num_features which is v(in paper), this is also used\n","  # for calculating distances\n","\n","\n","  bnd = [] # This is needed for l-bfgs algorithm\n","  for i, _ in enumerate(params):\n","      if i < num_features * 2 or i >= num_features * 2 + NUM_PROTOTYPES:\n","          bnd.append((None, None))\n","      else:\n","          bnd.append((0, 1))\n","\n","  new_params = optim.fmin_l_bfgs_b(optim_objective, x0=params, epsilon=1e-5,\n","                                    args=(data_sensitive, data_non_sensitive,\n","                                          y_sensitive, y_non_sensitive),\n","                                    bounds=bnd, approx_grad=True, maxfun=1_000,\n","                                    maxiter=1_000)[0]\n","\n","\n","  x_hat_senitive, x_hat_nons, y_hat_sens, y_hat_nons = optim_objective(new_params,data_sensitive, data_non_sensitive,\n","                                          y_sensitive, y_non_sensitive, inference=True)\n","\n","  FairP= np.hstack ([y_hat_sens[:, None],x_hat_senitive ] )\n","  FairNP= np.hstack  ([ y_hat_nons[:, None],x_hat_nons   ])\n","  #print(FairNP )\n","  #print(FairP)\n","\n","  FdfP= pd.DataFrame(FairP )\n","  #FdfP.to_csv(\"FairProtected.csv\")\n","  FdfNP= pd.DataFrame (FairNP )\n","  #FdfNP.to_csv(\"FairUnprotected.csv\")\n","  FairData= FdfP.append(FdfNP ,ignore_index= True)\n","  FairData.columns= list(data.columns)\n","\n","  FairData ['race']= (FairData['race'] >= FairData['race'].mean()).astype(int)\n","  FairData['first_pf']= (FairData['first_pf'] >= FairData['first_pf'].mean()).astype(int)\n","  path= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/LFR/FairData/Law/Train\"\n","\n","  TrainOuput= os.path.join(path ,(\"Train\" + str(count)+ \".csv\"))\n","  FairData.to_csv(TrainOuput, index= False )\n","  # FairData.to_csv(\"Train 1.csv\")\n","\n","\n","  # print(FairData )\n","  #FairData.to_csv(\"FairData.csv\")\n","  print ('Done')\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Dataset contains 17433 examples and 7 features\n","From which 2805 belong to sensitive group and 14628 to non nensitive group \n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-7915a2e2f56d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m                                           y_sensitive, y_non_sensitive),\n\u001b[1;32m     52\u001b[0m                                     \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapprox_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1_000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                                     maxiter=1_000)[0]\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 199\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    200\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    201\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_approx_fprime_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_approx_fprime_helper\u001b[0;34m(xk, f, epsilon, args, f0)\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0mei\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mei\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-c7ca244ea0c6>\u001b[0m in \u001b[0;36moptim_objective\u001b[0;34m(params, data_sensitive, data_non_sensitive, y_sensitive, y_non_sensitive, inference, NUM_PROTOTYPES, A_x, A_y, A_z, print_every)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mdists_sensitive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_sensitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_sensitive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mdists_non_sensitive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_non_sensitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_non_sensitive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# get probabilities of mappings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-c7ca244ea0c6>\u001b[0m in \u001b[0;36mdistances\u001b[0;34m(X, v, alpha)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mdist_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# multiplying by weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0msum_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mdists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"FhC_0CNsAuiI","colab":{"base_uri":"https://localhost:8080/","height":437},"executionInfo":{"status":"error","timestamp":1630050752369,"user_tz":-570,"elapsed":16982,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"272bf8ef-f731-4092-a711-8181d092a76e"},"source":["# seperation into sensitive and non sensitive\n","\n","\n","for count in range(1,51,1):\n","  Test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Law/Test'\n","  Test_path= os.path.join(Test_url ,(\"Test\"+ str(count)+ \".csv\"))\n","  data= pd.read_csv(Test_path).drop('region_first', axis=1)\n","  first_column = data.pop('first_pf')\n","  data.insert(0, 'first_pf', first_column)\n","\n","\n","  data_sensitive = data.loc[data.race==0]\n","  data_non_sensitive = data[data.race ==1]\n","  y_sensitive = data_sensitive.first_pf\n","  y_non_sensitive = data_non_sensitive.first_pf\n","\n","  print (f'Dataset contains {data.shape[0]} examples and {data.shape[1]} features')\n","  print (f'From which {data_sensitive.shape[0]} belong to sensitive group and {data_non_sensitive.shape[0]} to non nensitive group ')\n","\n","  del data_sensitive['first_pf']\n","  del data_non_sensitive['first_pf']\n","\n","  # Standard Scaling\n","  data_sensitive = StandardScaler().fit_transform(data_sensitive)\n","  data_non_sensitive = StandardScaler().fit_transform(data_non_sensitive)\n","\n","\n","  NUM_PROTOTYPES = 10\n","  num_features = data_sensitive.shape[1]\n","\n","  params = np.random.uniform(size=(num_features * 2 + NUM_PROTOTYPES + NUM_PROTOTYPES * num_features))\n","  # here we generate random weight for each of the features both for sensitive data\n","  # and for non sensitive, hence num_features*2(in paper this is denoted as alpha)\n","  # alphas are used for calculating distances\n","\n","  # Then NUM_PROTOTYPES is a weight for each prototype, this is multiplied with \n","  # M_nk s and used for calculating y_hat\n","\n","  # Next is NUM_PROTOTYPES * num_features which is v(in paper), this is also used\n","  # for calculating distances\n","\n","\n","  bnd = [] # This is needed for l-bfgs algorithm\n","  for i, _ in enumerate(params):\n","      if i < num_features * 2 or i >= num_features * 2 + NUM_PROTOTYPES:\n","          bnd.append((None, None))\n","      else:\n","          bnd.append((0, 1))\n","\n","  new_params = optim.fmin_l_bfgs_b(optim_objective, x0=params, epsilon=1e-5,\n","                                    args=(data_sensitive, data_non_sensitive,\n","                                          y_sensitive, y_non_sensitive),\n","                                    bounds=bnd, approx_grad=True, maxfun=1_000,\n","                                    maxiter=1_000)[0]\n","\n","\n","  x_hat_senitive, x_hat_nons, y_hat_sens, y_hat_nons = optim_objective(new_params,data_sensitive, data_non_sensitive,\n","                                          y_sensitive, y_non_sensitive, inference=True)\n","\n","  FairP= np.hstack ([y_hat_sens[:, None],x_hat_senitive ] )\n","  FairNP= np.hstack  ([ y_hat_nons[:, None],x_hat_nons   ])\n","  #print(FairNP )\n","  #print(FairP)\n","\n","  FdfP= pd.DataFrame(FairP )\n","  #FdfP.to_csv(\"FairProtected.csv\")\n","  FdfNP= pd.DataFrame (FairNP )\n","  #FdfNP.to_csv(\"FairUnprotected.csv\")\n","  FairData= FdfP.append(FdfNP ,ignore_index= True)\n","  FairData.columns= list(data.columns)\n","\n","  FairData ['race']= (FairData['race'] >= FairData['race'].mean()).astype(int)\n","  FairData['first_pf']= (FairData['first_pf'] >= FairData['first_pf'].mean()).astype(int)\n","  path= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/LFR/FairData/Law/Test\"\n","\n","  TestOuput= os.path.join(path ,(\"Test\" + str(count)+ \".csv\"))\n","  FairData.to_csv(TestOuput, index= False )\n","  # FairData.to_csv(\"Test 1.csv\")\n","\n","\n","  # print(FairData )\n","  #FairData.to_csv(\"FairData.csv\")\n","  print ('Done')\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Dataset contains 4359 examples and 7 features\n","From which 675 belong to sensitive group and 3684 to non nensitive group \n","loss on iteration 57900 : 245.2606140286314, L_x - 245.06924217825505 L_y - 0.11940872421518073 L_z - 0.07196312616116891\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-bd048504fd98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m                                           y_sensitive, y_non_sensitive),\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapprox_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxfun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1_000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     maxiter=1_000)[0]\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[0;32m--> 199\u001b[0;31m                            **opts)\n\u001b[0m\u001b[1;32m    200\u001b[0m     d = {'grad': res['jac'],\n\u001b[1;32m    201\u001b[0m          \u001b[0;34m'task'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_approx_fprime_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_approx_fprime_helper\u001b[0;34m(xk, f, epsilon, args, f0)\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0mei\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mei\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-c7ca244ea0c6>\u001b[0m in \u001b[0;36moptim_objective\u001b[0;34m(params, data_sensitive, data_non_sensitive, y_sensitive, y_non_sensitive, inference, NUM_PROTOTYPES, A_x, A_y, A_z, print_every)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mdists_sensitive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_sensitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_sensitive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mdists_non_sensitive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_non_sensitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_non_sensitive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# get probabilities of mappings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-c7ca244ea0c6>\u001b[0m in \u001b[0;36mdistances\u001b[0;34m(X, v, alpha)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# X = X.values  # converting to NumPy, this is needed in case you pass dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# squarred distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mdist_alpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# multiplying by weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0msum_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}