{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Violent.ipynb","provenance":[{"file_id":"1zUVqlZwnp2gUWV_U6552PNihQarJC9I9","timestamp":1628683226587},{"file_id":"1AJWD--4mn4SFR1x2nX38Hkscv0C2Ovqi","timestamp":1628499839227}],"authorship_tag":"ABX9TyMTXyPAQqebV+ql3Z57zFSC"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ff_Jv8Ptu4lP"},"source":["\n","# INSTALLATION"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q5qYRG8zufHw","executionInfo":{"status":"ok","timestamp":1629030607052,"user_tz":-570,"elapsed":13844,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"264661ff-6ed6-495e-804e-a028dfe46061"},"source":["!pip install aif360\n","!pip install fairlearn"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting aif360\n","  Downloading aif360-0.4.0-py3-none-any.whl (175 kB)\n","\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 24.9 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 19.2 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 61 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 92 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 102 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 112 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 122 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 133 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 143 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 153 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 163 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 174 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 175 kB 8.6 MB/s \n","\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.1.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from aif360) (3.2.2)\n","Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from aif360) (0.22.2.post1)\n","Collecting tempeh\n","  Downloading tempeh-0.1.12-py3-none-any.whl (39 kB)\n","Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.19.5)\n","Requirement already satisfied: scipy<1.6.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from aif360) (1.4.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->aif360) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->aif360) (1.0.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360) (1.3.1)\n","Collecting memory-profiler\n","  Downloading memory_profiler-0.58.0.tar.gz (36 kB)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360) (3.6.4)\n","Collecting shap\n","  Downloading shap-0.39.0.tar.gz (356 kB)\n","\u001b[K     |████████████████████████████████| 356 kB 41.5 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360) (2.23.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler->tempeh->aif360) (5.4.8)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (8.8.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (1.10.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (1.4.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (21.2.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (0.7.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360) (57.2.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360) (1.24.3)\n","Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360) (4.62.0)\n","Collecting slicer==0.0.7\n","  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n","Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360) (0.51.2)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360) (1.3.0)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap->tempeh->aif360) (0.34.0)\n","Building wheels for collected packages: memory-profiler, shap\n","  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for memory-profiler: filename=memory_profiler-0.58.0-py3-none-any.whl size=30190 sha256=b338631c0d531bd3a85f101a8ff0d1cb0aaf7ebefdc620c362ce342c0a82aee1\n","  Stored in directory: /root/.cache/pip/wheels/56/19/d5/8cad06661aec65a04a0d6785b1a5ad035cb645b1772a4a0882\n","  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for shap: filename=shap-0.39.0-cp37-cp37m-linux_x86_64.whl size=491652 sha256=fef1b5075a151d1f4de5a36fdff31922c83c3b4d1883fe8a1c2687f0599c6b54\n","  Stored in directory: /root/.cache/pip/wheels/ca/25/8f/6ae5df62c32651cd719e972e738a8aaa4a87414c4d2b14c9c0\n","Successfully built memory-profiler shap\n","Installing collected packages: slicer, shap, memory-profiler, tempeh, aif360\n","Successfully installed aif360-0.4.0 memory-profiler-0.58.0 shap-0.39.0 slicer-0.0.7 tempeh-0.1.12\n","Collecting fairlearn\n","  Downloading fairlearn-0.7.0-py3-none-any.whl (177 kB)\n","\u001b[K     |████████████████████████████████| 177 kB 8.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (1.19.5)\n","Requirement already satisfied: pandas>=0.25.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (1.1.5)\n","Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (0.22.2.post1)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn) (1.4.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.1->fairlearn) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.1->fairlearn) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.25.1->fairlearn) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->fairlearn) (1.0.1)\n","Installing collected packages: fairlearn\n","Successfully installed fairlearn-0.7.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TltW3iPkux0Q","executionInfo":{"status":"ok","timestamp":1629030620036,"user_tz":-570,"elapsed":1423,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"b43b0405-fdbe-48c8-d59e-09759aff1e83"},"source":["!apt-get install -jre\n","!java -version"],"execution_count":3,"outputs":[{"output_type":"stream","text":["E: Command line option 'j' [from -jre] is not understood in combination with the other options.\n","openjdk version \"11.0.11\" 2021-04-20\n","OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04)\n","OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KssrNl8GvDYU","executionInfo":{"status":"ok","timestamp":1629030652763,"user_tz":-570,"elapsed":32733,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"84de8dd4-bd7d-4628-b788-c71f6d801b8c"},"source":["!pip install h2o"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting h2o\n","  Downloading h2o-3.32.1.5.tar.gz (164.8 MB)\n","\u001b[K     |████████████████████████████████| 164.8 MB 2.1 kB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from h2o) (2.23.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from h2o) (0.8.9)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from h2o) (0.16.0)\n","Collecting colorama>=0.3.8\n","  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->h2o) (2021.5.30)\n","Building wheels for collected packages: h2o\n","  Building wheel for h2o (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for h2o: filename=h2o-3.32.1.5-py2.py3-none-any.whl size=164886106 sha256=eadd820638f489d783434d610dea4fb4852fe7722d910b1a358d704469bee831\n","  Stored in directory: /root/.cache/pip/wheels/2f/f4/f6/7115a720596f0b6c377b3d82c28242585c7bb7ab27d430f97c\n","Successfully built h2o\n","Installing collected packages: colorama, h2o\n","Successfully installed colorama-0.4.4 h2o-3.32.1.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_NQn2JJ0uw6u","executionInfo":{"status":"ok","timestamp":1629030655644,"user_tz":-570,"elapsed":2901,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"dd086e3b-4b11-49f9-cb81-b12220a0795b"},"source":["!pip install xlsxwriter"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting xlsxwriter\n","  Downloading XlsxWriter-3.0.1-py3-none-any.whl (148 kB)\n","\u001b[?25l\r\u001b[K     |██▏                             | 10 kB 30.8 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 20 kB 31.8 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 30 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 40 kB 16.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 51 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 61 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 71 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 81 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 92 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 102 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 112 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 122 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 133 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 143 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 148 kB 8.2 MB/s \n","\u001b[?25hInstalling collected packages: xlsxwriter\n","Successfully installed xlsxwriter-3.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a0YklbHpAxd8","executionInfo":{"status":"ok","timestamp":1629030660973,"user_tz":-570,"elapsed":5333,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"6205118c-6491-4df5-c03d-ddb35ae4f60a"},"source":["!pip install BlackBoxAuditing"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Collecting BlackBoxAuditing\n","  Downloading BlackBoxAuditing-0.1.54.tar.gz (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 7.6 MB/s \n","\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (2.6.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (3.2.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (1.1.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from BlackBoxAuditing) (1.19.5)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (1.3.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->BlackBoxAuditing) (0.10.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->BlackBoxAuditing) (1.15.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->BlackBoxAuditing) (2018.9)\n","Building wheels for collected packages: BlackBoxAuditing\n","  Building wheel for BlackBoxAuditing (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for BlackBoxAuditing: filename=BlackBoxAuditing-0.1.54-py2.py3-none-any.whl size=1394771 sha256=7d83754412934376011bbb5b7090a2e8070338d342ff54425645fe6a05976ecc\n","  Stored in directory: /root/.cache/pip/wheels/05/9f/ee/541a74be4cf5dad17430e64d3276370ea7b6a834a76cb4215a\n","Successfully built BlackBoxAuditing\n","Installing collected packages: BlackBoxAuditing\n","Successfully installed BlackBoxAuditing-0.1.54\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-Y_uQ6vdvN4a"},"source":["#IMPORTS"]},{"cell_type":"code","metadata":{"id":"rf1aISz6vGfR","executionInfo":{"status":"ok","timestamp":1629030669587,"user_tz":-570,"elapsed":8627,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":["import numpy as np\n","from mlxtend.feature_selection import  ExhaustiveFeatureSelector\n","from xgboost import  XGBClassifier\n","# import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import openpyxl\n","import xlsxwriter\n","from openpyxl import  load_workbook\n","\n","import BlackBoxAuditing\n","import shap\n","#suppress setwith copy warning\n","pd.set_option('mode.chained_assignment',None)\n","from sklearn.feature_selection import VarianceThreshold\n","from sklearn.feature_selection import SelectKBest, SelectFwe, SelectPercentile,SelectFdr, SelectFpr, SelectFromModel\n","from sklearn.feature_selection import chi2, mutual_info_classif\n","# from skfeature.function.similarity_based import fisher_score\n","import aif360\n","import matplotlib.pyplot as plt\n","from aif360.metrics.classification_metric import ClassificationMetric\n","\n","from aif360.metrics import BinaryLabelDatasetMetric\n","from aif360.algorithms.preprocessing import DisparateImpactRemover, Reweighing, LFR,OptimPreproc\n","from aif360.datasets import StandardDataset , BinaryLabelDataset\n","from sklearn.preprocessing import MinMaxScaler \n","MM= MinMaxScaler()\n","import h2o\n","from h2o.automl import H2OAutoML\n","from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n","\n","import sys\n","sys.path.append(\"../\")\n","import os\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":530},"id":"RcxQeeX7vUXz","executionInfo":{"status":"ok","timestamp":1629030676836,"user_tz":-570,"elapsed":7262,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"0f84135a-f18e-42fd-c3e8-f35503abef97"},"source":["h2o.init()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n","Attempting to start a local H2O server...\n","  Java Version: openjdk version \"11.0.11\" 2021-04-20; OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.18.04); OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.18.04, mixed mode, sharing)\n","  Starting server from /usr/local/lib/python3.7/dist-packages/h2o/backend/bin/h2o.jar\n","  Ice root: /tmp/tmp7h6tcgfx\n","  JVM stdout: /tmp/tmp7h6tcgfx/h2o_unknownUser_started_from_python.out\n","  JVM stderr: /tmp/tmp7h6tcgfx/h2o_unknownUser_started_from_python.err\n","  Server is running at http://127.0.0.1:54321\n","Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O_cluster_uptime:</td>\n","<td>03 secs</td></tr>\n","<tr><td>H2O_cluster_timezone:</td>\n","<td>Etc/UTC</td></tr>\n","<tr><td>H2O_data_parsing_timezone:</td>\n","<td>UTC</td></tr>\n","<tr><td>H2O_cluster_version:</td>\n","<td>3.32.1.5</td></tr>\n","<tr><td>H2O_cluster_version_age:</td>\n","<td>10 days </td></tr>\n","<tr><td>H2O_cluster_name:</td>\n","<td>H2O_from_python_unknownUser_7hub0g</td></tr>\n","<tr><td>H2O_cluster_total_nodes:</td>\n","<td>1</td></tr>\n","<tr><td>H2O_cluster_free_memory:</td>\n","<td>3.172 Gb</td></tr>\n","<tr><td>H2O_cluster_total_cores:</td>\n","<td>2</td></tr>\n","<tr><td>H2O_cluster_allowed_cores:</td>\n","<td>2</td></tr>\n","<tr><td>H2O_cluster_status:</td>\n","<td>accepting new members, healthy</td></tr>\n","<tr><td>H2O_connection_url:</td>\n","<td>http://127.0.0.1:54321</td></tr>\n","<tr><td>H2O_connection_proxy:</td>\n","<td>{\"http\": null, \"https\": null}</td></tr>\n","<tr><td>H2O_internal_security:</td>\n","<td>False</td></tr>\n","<tr><td>H2O_API_Extensions:</td>\n","<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n","<tr><td>Python_version:</td>\n","<td>3.7.11 final</td></tr></table></div>"],"text/plain":["--------------------------  ------------------------------------------------------------------\n","H2O_cluster_uptime:         03 secs\n","H2O_cluster_timezone:       Etc/UTC\n","H2O_data_parsing_timezone:  UTC\n","H2O_cluster_version:        3.32.1.5\n","H2O_cluster_version_age:    10 days\n","H2O_cluster_name:           H2O_from_python_unknownUser_7hub0g\n","H2O_cluster_total_nodes:    1\n","H2O_cluster_free_memory:    3.172 Gb\n","H2O_cluster_total_cores:    2\n","H2O_cluster_allowed_cores:  2\n","H2O_cluster_status:         accepting new members, healthy\n","H2O_connection_url:         http://127.0.0.1:54321\n","H2O_connection_proxy:       {\"http\": null, \"https\": null}\n","H2O_internal_security:      False\n","H2O_API_Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n","Python_version:             3.7.11 final\n","--------------------------  ------------------------------------------------------------------"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"RQVI-ISXvrZm"},"source":["#**************************LOADING DATASET*******************************"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FEGPULDrvk3g","executionInfo":{"status":"ok","timestamp":1629030760234,"user_tz":-570,"elapsed":38176,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}},"outputId":"013e2ccc-9177-4e2a-fd09-211c469dcdbb"},"source":["from google.colab import drive \n","drive.mount('/content/gdrive')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rA0wTXKH-csL","executionInfo":{"status":"aborted","timestamp":1629030719263,"user_tz":-570,"elapsed":671,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":["# tr=pd.read_csv(r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Test/Test50.csv')\n","# tr\n","# tester= BinaryLabelDataset(favorable_label=1,\n","#                                 unfavorable_label=0,\n","#                                 df=tr,\n","#                                 label_names=['two_year_recid'],\n","#                                 protected_attribute_names=['race'],\n","#                                 unprivileged_protected_attributes=[[0]],\n","#                                 privileged_protected_attributes=[[1]])\n","# tester\n","# DIR = DisparateImpactRemover(repair_level=1.0)\n","# DI_Test = DIR.fit_transform(tester)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6cp3EDkPEKY9"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"qDh3f5HwHubq"},"source":["# GBM REPAIR LEVEL 1\n","\n"]},{"cell_type":"code","metadata":{"id":"uN9VfZBAvxCj","executionInfo":{"status":"aborted","timestamp":1629030719269,"user_tz":-570,"elapsed":676,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":["for i in range(38,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","  #********************************************************binary labels for DI Remover*************************************************************\n","\n","  bldTrain= BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=train,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  \n","  bldTest=  BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=test,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  #*******************************************************DI Remover instance**************************************************************\n","  DIR = DisparateImpactRemover(repair_level=1)\n","\n","  DI_Train = DIR.fit_transform(bldTrain )\n","  DI_Test = DIR.fit_transform(bldTest)\n","  \n","  #*****************************************Repaired Train and Test Set*******************************************************\n","  train=  pd.DataFrame(np.hstack([DI_Train .labels,DI_Train .features]),columns=train.columns)\n","  test=  pd.DataFrame(np.hstack([DI_Test .labels,DI_Test .features]),columns= test.columns)\n","  # TotalRepairedDF= pd.concat([RepairedTrain ,RepairedTest ])\n","\n","\n","  # normalization of train and test sets\n","  Fitter= MM.fit(train)\n","  transformed_train=Fitter.transform(train)\n","  train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  #test normalization\n","  transformed_test=Fitter.transform(test)\n","  test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","  # *************CHECKING FAIRNESS IN DATASET**************************\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","\n","\n","  #Transforming the Train and Test Set to BinaryLabel\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  # class Train(StandardDataset):\n","  #     def __init__(self,label_name= 'two_year_recid',\n","  #                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","  #         super(Train, self).__init__(df=train  , label_name=label_name ,\n","  #             favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","  #             privileged_classes=privileged_classes ,\n","  #            )\n","\n","\n","\n","\n","\n","  # BLD_Train= Train(protected_attribute_names= ['race'],\n","  #                        privileged_classes= [[1]])\n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  ## ********************Checking Bias Repaired Data********************************\n","  DataBias_Checker = BinaryLabelDatasetMetric(BLD_Test    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","\n","  dsp= DataBias_Checker .statistical_parity_difference() \n","  dif= DataBias_Checker.consistency()  \n","  ddi= DataBias_Checker.disparate_impact() \n","\n","  print('The Statistical Parity diference is = {diff}'.format(diff=  dsp  ))\n","  print('Individual Fairness is = {IF}'.format( IF=  dif ))\n","  print('Disparate Impact is = {IF}'.format( IF=   ddi  ))\n","  # ********************SETTING TO H20 FRAME AND MODEL TRAINING*******************************\n","  x = list(train.columns)\n","  y = \"two_year_recid\"\n","  x.remove(y)\n","  Train=h2o.H2OFrame(train)\n","  Test= h2o.H2OFrame(test)\n","  Train[y] = Train[y].asfactor()\n","  Test[y] = Test[y].asfactor()\n","  aml = H2OAutoML(max_models=10, nfolds=10, include_algos=['GBM'] , stopping_metric='AUTO') #verbosity='info',,'GBM', 'DRF'\n","  aml.train(x=x, y=y, training_frame=Train)\n","  best_model= aml.leader\n","  # a.model_performance()\n","  #**********************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS****************************\n","  #predicted labels\n","  gbm_Predictions= best_model.predict(Test)\n","  gbm_Predictions= gbm_Predictions.as_data_frame()\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= gbm_Predictions.predict.to_numpy()\n","\n","  # ********************COMPUTE DISCRIMINATION*****************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","  # # Workbook= pd.ExcelFile(r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/BaseLines/GBM/gbm_Results.xlsx')\n","  # excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/BaseLines/GBM/gbm_Results.xlsx')\n","  # OldDF= excelBook.get_sheet_by_name(\"Violent\")#pd.read_excel(Workbook,sheet_name='Violent')\n","  #load workbook\n","  excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_1/GBM/gbm_Results.xlsx')\n","  Violent= excelBook['Violent']\n","  data= Violent.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n","  10# Create a DataFrame based on the second and subsequent lines of data\n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF,\n","                                          'DATA_SP':dsp,'DATA_CONS':dif,'DATA_DI':ddi})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_1/GBM/gbm_Results.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Violent', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4H3vvxWBboi5"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"AqKxt33Nb4Tj"},"source":["# GBM REPAIR LEVEL 0.7\n","\n"]},{"cell_type":"code","metadata":{"id":"fFeDSQUnb4Tj","executionInfo":{"status":"aborted","timestamp":1629030719271,"user_tz":-570,"elapsed":678,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":["for i in range(21,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","  #********************************************************binary labels for DI Remover*************************************************************\n","\n","  bldTrain= BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=train,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  \n","  bldTest=  BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=test,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  #*******************************************************DI Remover instance**************************************************************\n","  DIR = DisparateImpactRemover(repair_level=0.7)\n","\n","  DI_Train = DIR.fit_transform(bldTrain )\n","  DI_Test = DIR.fit_transform(bldTest)\n","  \n","  #*****************************************Repaired Train and Test Set*******************************************************\n","  train=  pd.DataFrame(np.hstack([DI_Train .labels,DI_Train .features]),columns=train.columns)\n","  test=  pd.DataFrame(np.hstack([DI_Test .labels,DI_Test .features]),columns= test.columns)\n","  # TotalRepairedDF= pd.concat([RepairedTrain ,RepairedTest ])\n","\n","\n","  # normalization of train and test sets\n","  Fitter= MM.fit(train)\n","  transformed_train=Fitter.transform(train)\n","  train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  #test normalization\n","  transformed_test=Fitter.transform(test)\n","  test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","  # *************CHECKING FAIRNESS IN DATASET**************************\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","\n","\n","  #Transforming the Train and Test Set to BinaryLabel\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  # class Train(StandardDataset):\n","  #     def __init__(self,label_name= 'two_year_recid',\n","  #                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","  #         super(Train, self).__init__(df=train  , label_name=label_name ,\n","  #             favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","  #             privileged_classes=privileged_classes ,\n","  #            )\n","\n","\n","\n","\n","\n","  # BLD_Train= Train(protected_attribute_names= ['race'],\n","  #                        privileged_classes= [[1]])\n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  ## ********************Checking Bias Repaired Data********************************\n","  DataBias_Checker = BinaryLabelDatasetMetric(BLD_Test    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","\n","  dsp= DataBias_Checker .statistical_parity_difference() \n","  dif= DataBias_Checker.consistency()  \n","  ddi= DataBias_Checker.disparate_impact() \n","\n","  print('The Statistical Parity diference is = {diff}'.format(diff=  dsp  ))\n","  print('Individual Fairness is = {IF}'.format( IF=  dif ))\n","  print('Disparate Impact is = {IF}'.format( IF=   ddi  ))\n","  # ********************SETTING TO H20 FRAME AND MODEL TRAINING*******************************\n","  x = list(train.columns)\n","  y = \"two_year_recid\"\n","  x.remove(y)\n","  Train=h2o.H2OFrame(train)\n","  Test= h2o.H2OFrame(test)\n","  Train[y] = Train[y].asfactor()\n","  Test[y] = Test[y].asfactor()\n","  aml = H2OAutoML(max_models=10, nfolds=10, include_algos=['GBM'] , stopping_metric='AUTO') #verbosity='info',,'GBM', 'DRF'\n","  aml.train(x=x, y=y, training_frame=Train)\n","  best_model= aml.leader\n","  # a.model_performance()\n","  #**********************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS****************************\n","  #predicted labels\n","  gbm_Predictions= best_model.predict(Test)\n","  gbm_Predictions= gbm_Predictions.as_data_frame()\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= gbm_Predictions.predict.to_numpy()\n","\n","  # ********************COMPUTE DISCRIMINATION*****************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","  # # Workbook= pd.ExcelFile(r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/BaseLines/GBM/gbm_Results.xlsx')\n","  # excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/BaseLines/GBM/gbm_Results.xlsx')\n","  # OldDF= excelBook.get_sheet_by_name(\"Violent\")#pd.read_excel(Workbook,sheet_name='Violent')\n","  #load workbook\n","  excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_p7/GBM/gbm_Results.xlsx')\n","  Violent= excelBook['Adult']\n","  data= Violent.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n","  10# Create a DataFrame based on the second and subsequent lines of data\n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF,\n","                                          'DATA_SP':dsp,'DATA_CONS':dif,'DATA_DI':ddi})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_p7/GBM/gbm_Results.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Adult', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lyYm_xneb8ID","executionInfo":{"status":"aborted","timestamp":1629030719273,"user_tz":-570,"elapsed":680,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RvqC1Pd5b8gF"},"source":["# GBM REPAIR LEVEL 0.5\n","\n"]},{"cell_type":"code","metadata":{"id":"1ayeRQ9ib8gG","executionInfo":{"status":"aborted","timestamp":1629030719273,"user_tz":-570,"elapsed":680,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":["for i in range(1,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","  #********************************************************binary labels for DI Remover*************************************************************\n","\n","  bldTrain= BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=train,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  \n","  bldTest=  BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=test,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  #*******************************************************DI Remover instance**************************************************************\n","  DIR = DisparateImpactRemover(repair_level=0.5)\n","\n","  DI_Train = DIR.fit_transform(bldTrain )\n","  DI_Test = DIR.fit_transform(bldTest)\n","  \n","  #*****************************************Repaired Train and Test Set*******************************************************\n","  train=  pd.DataFrame(np.hstack([DI_Train .labels,DI_Train .features]),columns=train.columns)\n","  test=  pd.DataFrame(np.hstack([DI_Test .labels,DI_Test .features]),columns= test.columns)\n","  # TotalRepairedDF= pd.concat([RepairedTrain ,RepairedTest ])\n","\n","\n","  # normalization of train and test sets\n","  Fitter= MM.fit(train)\n","  transformed_train=Fitter.transform(train)\n","  train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  #test normalization\n","  transformed_test=Fitter.transform(test)\n","  test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","  # *************CHECKING FAIRNESS IN DATASET**************************\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","\n","\n","  #Transforming the Train and Test Set to BinaryLabel\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  # class Train(StandardDataset):\n","  #     def __init__(self,label_name= 'two_year_recid',\n","  #                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","  #         super(Train, self).__init__(df=train  , label_name=label_name ,\n","  #             favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","  #             privileged_classes=privileged_classes ,\n","  #            )\n","\n","\n","\n","\n","\n","  # BLD_Train= Train(protected_attribute_names= ['race'],\n","  #                        privileged_classes= [[1]])\n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  ## ********************Checking Bias Repaired Data********************************\n","  DataBias_Checker = BinaryLabelDatasetMetric(BLD_Test    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","\n","  dsp= DataBias_Checker .statistical_parity_difference() \n","  dif= DataBias_Checker.consistency()  \n","  ddi= DataBias_Checker.disparate_impact() \n","\n","  print('The Statistical Parity diference is = {diff}'.format(diff=  dsp  ))\n","  print('Individual Fairness is = {IF}'.format( IF=  dif ))\n","  print('Disparate Impact is = {IF}'.format( IF=   ddi  ))\n","  # ********************SETTING TO H20 FRAME AND MODEL TRAINING*******************************\n","  x = list(train.columns)\n","  y = \"two_year_recid\"\n","  x.remove(y)\n","  Train=h2o.H2OFrame(train)\n","  Test= h2o.H2OFrame(test)\n","  Train[y] = Train[y].asfactor()\n","  Test[y] = Test[y].asfactor()\n","  aml = H2OAutoML(max_models=10, nfolds=10, include_algos=['GBM'] , stopping_metric='AUTO') #verbosity='info',,'GBM', 'DRF'\n","  aml.train(x=x, y=y, training_frame=Train)\n","  best_model= aml.leader\n","  # a.model_performance()\n","  #**********************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS****************************\n","  #predicted labels\n","  gbm_Predictions= best_model.predict(Test)\n","  gbm_Predictions= gbm_Predictions.as_data_frame()\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= gbm_Predictions.predict.to_numpy()\n","\n","  # ********************COMPUTE DISCRIMINATION*****************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","  # # Workbook= pd.ExcelFile(r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/BaseLines/GBM/gbm_Results.xlsx')\n","  # excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/BaseLines/GBM/gbm_Results.xlsx')\n","  # OldDF= excelBook.get_sheet_by_name(\"Violent\")#pd.read_excel(Workbook,sheet_name='Violent')\n","  #load workbook\n","  excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_p5/GBM/gbm_Results.xlsx')\n","  Violent= excelBook['Violent']\n","  data= Violent.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n","  10# Create a DataFrame based on the second and subsequent lines of data\n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF,\n","                                          'DATA_SP':dsp,'DATA_CONS':dif,'DATA_DI':ddi})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_p5/GBM/gbm_Results.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Violent', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8j6QC5gvb_ES"},"source":["# GBM REPAIR LEVEL 0.3\n","\n"]},{"cell_type":"code","metadata":{"id":"K-KMLinTb_ET","executionInfo":{"status":"aborted","timestamp":1629030719274,"user_tz":-570,"elapsed":681,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":["for i in range(8,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","  #********************************************************binary labels for DI Remover*************************************************************\n","\n","  bldTrain= BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=train,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  \n","  bldTest=  BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=test,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  #*******************************************************DI Remover instance**************************************************************\n","  DIR = DisparateImpactRemover(repair_level=0.3)\n","\n","  DI_Train = DIR.fit_transform(bldTrain )\n","  DI_Test = DIR.fit_transform(bldTest)\n","  \n","  #*****************************************Repaired Train and Test Set*******************************************************\n","  train=  pd.DataFrame(np.hstack([DI_Train .labels,DI_Train .features]),columns=train.columns)\n","  test=  pd.DataFrame(np.hstack([DI_Test .labels,DI_Test .features]),columns= test.columns)\n","  # TotalRepairedDF= pd.concat([RepairedTrain ,RepairedTest ])\n","\n","\n","  # normalization of train and test sets\n","  Fitter= MM.fit(train)\n","  transformed_train=Fitter.transform(train)\n","  train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  #test normalization\n","  transformed_test=Fitter.transform(test)\n","  test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","  # *************CHECKING FAIRNESS IN DATASET**************************\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","\n","\n","  #Transforming the Train and Test Set to BinaryLabel\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  # class Train(StandardDataset):\n","  #     def __init__(self,label_name= 'two_year_recid',\n","  #                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","  #         super(Train, self).__init__(df=train  , label_name=label_name ,\n","  #             favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","  #             privileged_classes=privileged_classes ,\n","  #            )\n","\n","\n","\n","\n","\n","  # BLD_Train= Train(protected_attribute_names= ['race'],\n","  #                        privileged_classes= [[1]])\n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  ## ********************Checking Bias Repaired Data********************************\n","  DataBias_Checker = BinaryLabelDatasetMetric(BLD_Test    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","\n","  dsp= DataBias_Checker .statistical_parity_difference() \n","  dif= DataBias_Checker.consistency()  \n","  ddi= DataBias_Checker.disparate_impact() \n","\n","  print('The Statistical Parity diference is = {diff}'.format(diff=  dsp  ))\n","  print('Individual Fairness is = {IF}'.format( IF=  dif ))\n","  print('Disparate Impact is = {IF}'.format( IF=   ddi  ))\n","  # ********************SETTING TO H20 FRAME AND MODEL TRAINING*******************************\n","  x = list(train.columns)\n","  y = \"two_year_recid\"\n","  x.remove(y)\n","  Train=h2o.H2OFrame(train)\n","  Test= h2o.H2OFrame(test)\n","  Train[y] = Train[y].asfactor()\n","  Test[y] = Test[y].asfactor()\n","  aml = H2OAutoML(max_models=10, nfolds=10, include_algos=['GBM'] , stopping_metric='AUTO') #verbosity='info',,'GBM', 'DRF'\n","  aml.train(x=x, y=y, training_frame=Train)\n","  best_model= aml.leader\n","  # a.model_performance()\n","  #**********************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS****************************\n","  #predicted labels\n","  gbm_Predictions= best_model.predict(Test)\n","  gbm_Predictions= gbm_Predictions.as_data_frame()\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= gbm_Predictions.predict.to_numpy()\n","\n","  # ********************COMPUTE DISCRIMINATION*****************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","  # # Workbook= pd.ExcelFile(r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/BaseLines/GBM/gbm_Results.xlsx')\n","  # excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/BaseLines/GBM/gbm_Results.xlsx')\n","  # OldDF= excelBook.get_sheet_by_name(\"Violent\")#pd.read_excel(Workbook,sheet_name='Violent')\n","  #load workbook\n","  excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_p3/GBM/gbm_Results.xlsx')\n","  Violent= excelBook['Violent']\n","  data= Violent.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n","  10# Create a DataFrame based on the second and subsequent lines of data\n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF,\n","                                          'DATA_SP':dsp,'DATA_CONS':dif,'DATA_DI':ddi})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_p3/GBM/gbm_Results.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Violent', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cbSeeu-DcCEQ"},"source":["# GBM REPAIR LEVEL 0\n","\n"]},{"cell_type":"code","metadata":{"id":"58pOr1C6cCER","executionInfo":{"status":"aborted","timestamp":1629030719274,"user_tz":-570,"elapsed":681,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":["for i in range(25,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","  #********************************************************binary labels for DI Remover*************************************************************\n","\n","  bldTrain= BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=train,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  \n","  bldTest=  BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=test,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  #*******************************************************DI Remover instance**************************************************************\n","  DIR = DisparateImpactRemover(repair_level=0)\n","\n","  DI_Train = DIR.fit_transform(bldTrain )\n","  DI_Test = DIR.fit_transform(bldTest)\n","  \n","  #*****************************************Repaired Train and Test Set*******************************************************\n","  train=  pd.DataFrame(np.hstack([DI_Train .labels,DI_Train .features]),columns=train.columns)\n","  test=  pd.DataFrame(np.hstack([DI_Test .labels,DI_Test .features]),columns= test.columns)\n","  # TotalRepairedDF= pd.concat([RepairedTrain ,RepairedTest ])\n","\n","\n","  # normalization of train and test sets\n","  Fitter= MM.fit(train)\n","  transformed_train=Fitter.transform(train)\n","  train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  #test normalization\n","  transformed_test=Fitter.transform(test)\n","  test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","  # *************CHECKING FAIRNESS IN DATASET**************************\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","\n","\n","  #Transforming the Train and Test Set to BinaryLabel\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  # class Train(StandardDataset):\n","  #     def __init__(self,label_name= 'two_year_recid',\n","  #                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","  #         super(Train, self).__init__(df=train  , label_name=label_name ,\n","  #             favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","  #             privileged_classes=privileged_classes ,\n","  #            )\n","\n","\n","\n","\n","\n","  # BLD_Train= Train(protected_attribute_names= ['race'],\n","  #                        privileged_classes= [[1]])\n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  ## ********************Checking Bias Repaired Data********************************\n","  DataBias_Checker = BinaryLabelDatasetMetric(BLD_Test    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","\n","  dsp= DataBias_Checker .statistical_parity_difference() \n","  dif= DataBias_Checker.consistency()  \n","  ddi= DataBias_Checker.disparate_impact() \n","\n","  print('The Statistical Parity diference is = {diff}'.format(diff=  dsp  ))\n","  print('Individual Fairness is = {IF}'.format( IF=  dif ))\n","  print('Disparate Impact is = {IF}'.format( IF=   ddi  ))\n","  # ********************SETTING TO H20 FRAME AND MODEL TRAINING*******************************\n","  x = list(train.columns)\n","  y = \"two_year_recid\"\n","  x.remove(y)\n","  Train=h2o.H2OFrame(train)\n","  Test= h2o.H2OFrame(test)\n","  Train[y] = Train[y].asfactor()\n","  Test[y] = Test[y].asfactor()\n","  aml = H2OAutoML(max_models=10, nfolds=10, include_algos=['GBM'] , stopping_metric='AUTO') #verbosity='info',,'GBM', 'DRF'\n","  aml.train(x=x, y=y, training_frame=Train)\n","  best_model= aml.leader\n","  # a.model_performance()\n","  #**********************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS****************************\n","  #predicted labels\n","  gbm_Predictions= best_model.predict(Test)\n","  gbm_Predictions= gbm_Predictions.as_data_frame()\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= gbm_Predictions.predict.to_numpy()\n","\n","  # ********************COMPUTE DISCRIMINATION*****************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","  # # Workbook= pd.ExcelFile(r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/BaseLines/GBM/gbm_Results.xlsx')\n","  # excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/BaseLines/GBM/gbm_Results.xlsx')\n","  # OldDF= excelBook.get_sheet_by_name(\"Violent\")#pd.read_excel(Workbook,sheet_name='Violent')\n","  #load workbook\n","  excelBook= load_workbook('/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_p0/GBM/gbm_Results.xlsx')\n","  Violent= excelBook['Violent']\n","  data= Violent.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n","  10# Create a DataFrame based on the second and subsequent lines of data\n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF,\n","                                          'DATA_SP':dsp,'DATA_CONS':dif,'DATA_DI':ddi})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_p0/GBM/gbm_Results.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Violent', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hoc62jY7Olbt"},"source":["#LOGISTIC REGRESSION REPAIR LEVEL 1\n"]},{"cell_type":"code","metadata":{"id":"RZmY0q8iVY3O","executionInfo":{"status":"aborted","timestamp":1629030719275,"user_tz":-570,"elapsed":682,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":["for i in range(1,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","   #********************************************************binary labels for DI Remover*************************************************************\n","\n","  bldTrain= BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=train,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  \n","  bldTest=  BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=test,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  #*******************************************************DI Remover instance**************************************************************\n","  DIR = DisparateImpactRemover(repair_level=1)\n","\n","  DI_Train = DIR.fit_transform(bldTrain )\n","  DI_Test = DIR.fit_transform(bldTest)\n","  \n","  #*****************************************Repaired Train and Test Set*******************************************************\n","  train=  pd.DataFrame(np.hstack([DI_Train .labels,DI_Train .features]),columns=train.columns)\n","  test=  pd.DataFrame(np.hstack([DI_Test .labels,DI_Test .features]),columns= test.columns)\n","  # TotalRepairedDF= pd.concat([RepairedTrain ,RepairedTest ])\n","\n","  # normalization of train and test sets\n","  Fitter= MM.fit(train)\n","  transformed_train=Fitter.transform(train)\n","  train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  #test normalization\n","  transformed_test=Fitter.transform(test)\n","  test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","  # *************CHECKING FAIRNESS IN DATASET**************************\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","\n","\n","  #Transforming the Train and Test Set to BinaryLabel\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  # class Train(StandardDataset):\n","  #     def __init__(self,label_name= 'two_year_recid',\n","  #                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","  #         super(Train, self).__init__(df=train  , label_name=label_name ,\n","  #             favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","  #             privileged_classes=privileged_classes ,\n","  #            )\n","\n","\n","\n","\n","\n","  # BLD_Train= Train(protected_attribute_names= ['race'],\n","  #                        privileged_classes= [[1]])\n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  ## ********************Checking Bias in Data********************************\n","  DataBias_Checker = BinaryLabelDatasetMetric(BLD_Test    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","\n","  dsp= DataBias_Checker .statistical_parity_difference() \n","  dif= DataBias_Checker.consistency()  \n","  ddi= DataBias_Checker.disparate_impact() \n","\n","  print('The Statistical Parity diference is = {diff}'.format(diff=  dsp  ))\n","  print('Individual Fairness is = {IF}'.format( IF=  dif ))\n","  print('Disparate Impact is = {IF}'.format( IF=   ddi  ))\n","  # ********************SETTING TO H20 FRAME AND MODEL TRAINING*******************************\n","  x = list(train.columns)\n","  y = \"two_year_recid\"\n","  x.remove(y)\n","  Train=h2o.H2OFrame(train)\n","  Test= h2o.H2OFrame(test)\n","  Train[y] = Train[y].asfactor()\n","  Test[y] = Test[y].asfactor()\n","  LogReg = H2OGeneralizedLinearEstimator(family= \"binomial\", lambda_ = 0)\n","  LogReg.train(x=x, y=y, training_frame=Train)\n","\n","  LogReg_Predictions= LogReg.predict(Test)\n","  LogReg_Predictions= LogReg_Predictions.as_data_frame()\n","  # *************************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS**************************************\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= LogReg_Predictions.predict.to_numpy()\n","\n","  # ***************************COMPUTE DISCRIMINATION********************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","\n","  excelBook= load_workbook(r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_1/LogReg/LR_Results.xlsx')\n","  Violent= excelBook['Violent']\n","  data= Violent.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n"," \n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF,\n","                                          'DATA_SP':dsp,'DATA_CONS':dif,'DATA_DI':ddi})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_1/LogReg/LR_Results.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Violent', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"opLnVy5E7cYq","executionInfo":{"status":"aborted","timestamp":1629030719278,"user_tz":-570,"elapsed":685,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mrNs2FhAf7lg"},"source":["#LOGISTIC REGRESSION REPAIR LEVEL 0.7\n"]},{"cell_type":"code","metadata":{"id":"7FerMJoqf7lp","executionInfo":{"status":"aborted","timestamp":1629030719279,"user_tz":-570,"elapsed":23,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":["for i in range(1,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","   #********************************************************binary labels for DI Remover*************************************************************\n","\n","  bldTrain= BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=train,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  \n","  bldTest=  BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=test,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  #*******************************************************DI Remover instance**************************************************************\n","  DIR = DisparateImpactRemover(repair_level=0.7)\n","\n","  DI_Train = DIR.fit_transform(bldTrain )\n","  DI_Test = DIR.fit_transform(bldTest)\n","  \n","  #*****************************************Repaired Train and Test Set*******************************************************\n","  train=  pd.DataFrame(np.hstack([DI_Train .labels,DI_Train .features]),columns=train.columns)\n","  test=  pd.DataFrame(np.hstack([DI_Test .labels,DI_Test .features]),columns= test.columns)\n","  # TotalRepairedDF= pd.concat([RepairedTrain ,RepairedTest ])\n","\n","  # normalization of train and test sets\n","  Fitter= MM.fit(train)\n","  transformed_train=Fitter.transform(train)\n","  train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  #test normalization\n","  transformed_test=Fitter.transform(test)\n","  test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","  # *************CHECKING FAIRNESS IN DATASET**************************\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","\n","\n","  #Transforming the Train and Test Set to BinaryLabel\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  # class Train(StandardDataset):\n","  #     def __init__(self,label_name= 'two_year_recid',\n","  #                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","  #         super(Train, self).__init__(df=train  , label_name=label_name ,\n","  #             favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","  #             privileged_classes=privileged_classes ,\n","  #            )\n","\n","\n","\n","\n","\n","  # BLD_Train= Train(protected_attribute_names= ['race'],\n","  #                        privileged_classes= [[1]])\n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  ## ********************Checking Bias in Data********************************\n","  DataBias_Checker = BinaryLabelDatasetMetric(BLD_Test    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","\n","  dsp= DataBias_Checker .statistical_parity_difference() \n","  dif= DataBias_Checker.consistency()  \n","  ddi= DataBias_Checker.disparate_impact() \n","\n","  print('The Statistical Parity diference is = {diff}'.format(diff=  dsp  ))\n","  print('Individual Fairness is = {IF}'.format( IF=  dif ))\n","  print('Disparate Impact is = {IF}'.format( IF=   ddi  ))\n","  # ********************SETTING TO H20 FRAME AND MODEL TRAINING*******************************\n","  x = list(train.columns)\n","  y = \"two_year_recid\"\n","  x.remove(y)\n","  Train=h2o.H2OFrame(train)\n","  Test= h2o.H2OFrame(test)\n","  Train[y] = Train[y].asfactor()\n","  Test[y] = Test[y].asfactor()\n","  LogReg = H2OGeneralizedLinearEstimator(family= \"binomial\", lambda_ = 0)\n","  LogReg.train(x=x, y=y, training_frame=Train)\n","\n","  LogReg_Predictions= LogReg.predict(Test)\n","  LogReg_Predictions= LogReg_Predictions.as_data_frame()\n","  # *************************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS**************************************\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= LogReg_Predictions.predict.to_numpy()\n","\n","  # ***************************COMPUTE DISCRIMINATION********************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","\n","  excelBook= load_workbook(r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_p7/LogReg/LR_Results.xlsx')\n","  Violent= excelBook['Violent']\n","  data= Violent.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n"," \n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF,\n","                                          'DATA_SP':dsp,'DATA_CONS':dif,'DATA_DI':ddi})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_p7/LogReg/LR_Results.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Violent', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wlt5LqkFf8Au"},"source":["#LOGISTIC REGRESSION REPAIR LEVEL 0.5\n"]},{"cell_type":"code","metadata":{"id":"km2q3FDmf8Au","executionInfo":{"status":"aborted","timestamp":1629030719279,"user_tz":-570,"elapsed":22,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":["for i in range(1,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","   #********************************************************binary labels for DI Remover*************************************************************\n","\n","  bldTrain= BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=train,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  \n","  bldTest=  BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=test,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  #*******************************************************DI Remover instance**************************************************************\n","  DIR = DisparateImpactRemover(repair_level=0.5)\n","\n","  DI_Train = DIR.fit_transform(bldTrain )\n","  DI_Test = DIR.fit_transform(bldTest)\n","  \n","  #*****************************************Repaired Train and Test Set*******************************************************\n","  train=  pd.DataFrame(np.hstack([DI_Train .labels,DI_Train .features]),columns=train.columns)\n","  test=  pd.DataFrame(np.hstack([DI_Test .labels,DI_Test .features]),columns= test.columns)\n","  # TotalRepairedDF= pd.concat([RepairedTrain ,RepairedTest ])\n","\n","  # normalization of train and test sets\n","  Fitter= MM.fit(train)\n","  transformed_train=Fitter.transform(train)\n","  train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  #test normalization\n","  transformed_test=Fitter.transform(test)\n","  test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","  # *************CHECKING FAIRNESS IN DATASET**************************\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","\n","\n","  #Transforming the Train and Test Set to BinaryLabel\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  # class Train(StandardDataset):\n","  #     def __init__(self,label_name= 'two_year_recid',\n","  #                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","  #         super(Train, self).__init__(df=train  , label_name=label_name ,\n","  #             favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","  #             privileged_classes=privileged_classes ,\n","  #            )\n","\n","\n","\n","\n","\n","  # BLD_Train= Train(protected_attribute_names= ['race'],\n","  #                        privileged_classes= [[1]])\n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  ## ********************Checking Bias in Data********************************\n","  DataBias_Checker = BinaryLabelDatasetMetric(BLD_Test    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","\n","  dsp= DataBias_Checker .statistical_parity_difference() \n","  dif= DataBias_Checker.consistency()  \n","  ddi= DataBias_Checker.disparate_impact() \n","\n","  print('The Statistical Parity diference is = {diff}'.format(diff=  dsp  ))\n","  print('Individual Fairness is = {IF}'.format( IF=  dif ))\n","  print('Disparate Impact is = {IF}'.format( IF=   ddi  ))\n","  # ********************SETTING TO H20 FRAME AND MODEL TRAINING*******************************\n","  x = list(train.columns)\n","  y = \"two_year_recid\"\n","  x.remove(y)\n","  Train=h2o.H2OFrame(train)\n","  Test= h2o.H2OFrame(test)\n","  Train[y] = Train[y].asfactor()\n","  Test[y] = Test[y].asfactor()\n","  LogReg = H2OGeneralizedLinearEstimator(family= \"binomial\", lambda_ = 0)\n","  LogReg.train(x=x, y=y, training_frame=Train)\n","\n","  LogReg_Predictions= LogReg.predict(Test)\n","  LogReg_Predictions= LogReg_Predictions.as_data_frame()\n","  # *************************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS**************************************\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= LogReg_Predictions.predict.to_numpy()\n","\n","  # ***************************COMPUTE DISCRIMINATION********************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","\n","  excelBook= load_workbook(r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_p5/LogReg/LR_Results.xlsx')\n","  Violent= excelBook['Violent']\n","  data= Violent.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n"," \n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF,\n","                                          'DATA_SP':dsp,'DATA_CONS':dif,'DATA_DI':ddi})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_p5/LogReg/LR_Results.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Violent', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5QkgkqVtf8b9"},"source":["#LOGISTIC REGRESSION REPAIR LEVEL 0.3\n"]},{"cell_type":"code","metadata":{"id":"WEfwgNNKf8b9","executionInfo":{"status":"aborted","timestamp":1629030719280,"user_tz":-570,"elapsed":23,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":["for i in range(1,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","   #********************************************************binary labels for DI Remover*************************************************************\n","\n","  bldTrain= BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=train,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  \n","  bldTest=  BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=test,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  #*******************************************************DI Remover instance**************************************************************\n","  DIR = DisparateImpactRemover(repair_level=0.3)\n","\n","  DI_Train = DIR.fit_transform(bldTrain )\n","  DI_Test = DIR.fit_transform(bldTest)\n","  \n","  #*****************************************Repaired Train and Test Set*******************************************************\n","  train=  pd.DataFrame(np.hstack([DI_Train .labels,DI_Train .features]),columns=train.columns)\n","  test=  pd.DataFrame(np.hstack([DI_Test .labels,DI_Test .features]),columns= test.columns)\n","  # TotalRepairedDF= pd.concat([RepairedTrain ,RepairedTest ])\n","\n","  # normalization of train and test sets\n","  Fitter= MM.fit(train)\n","  transformed_train=Fitter.transform(train)\n","  train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  #test normalization\n","  transformed_test=Fitter.transform(test)\n","  test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","  # *************CHECKING FAIRNESS IN DATASET**************************\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","\n","\n","  #Transforming the Train and Test Set to BinaryLabel\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  # class Train(StandardDataset):\n","  #     def __init__(self,label_name= 'two_year_recid',\n","  #                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","  #         super(Train, self).__init__(df=train  , label_name=label_name ,\n","  #             favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","  #             privileged_classes=privileged_classes ,\n","  #            )\n","\n","\n","\n","\n","\n","  # BLD_Train= Train(protected_attribute_names= ['race'],\n","  #                        privileged_classes= [[1]])\n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  ## ********************Checking Bias in Data********************************\n","  DataBias_Checker = BinaryLabelDatasetMetric(BLD_Test    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","\n","  dsp= DataBias_Checker .statistical_parity_difference() \n","  dif= DataBias_Checker.consistency()  \n","  ddi= DataBias_Checker.disparate_impact() \n","\n","  print('The Statistical Parity diference is = {diff}'.format(diff=  dsp  ))\n","  print('Individual Fairness is = {IF}'.format( IF=  dif ))\n","  print('Disparate Impact is = {IF}'.format( IF=   ddi  ))\n","  # ********************SETTING TO H20 FRAME AND MODEL TRAINING*******************************\n","  x = list(train.columns)\n","  y = \"two_year_recid\"\n","  x.remove(y)\n","  Train=h2o.H2OFrame(train)\n","  Test= h2o.H2OFrame(test)\n","  Train[y] = Train[y].asfactor()\n","  Test[y] = Test[y].asfactor()\n","  LogReg = H2OGeneralizedLinearEstimator(family= \"binomial\", lambda_ = 0)\n","  LogReg.train(x=x, y=y, training_frame=Train)\n","\n","  LogReg_Predictions= LogReg.predict(Test)\n","  LogReg_Predictions= LogReg_Predictions.as_data_frame()\n","  # *************************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS**************************************\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= LogReg_Predictions.predict.to_numpy()\n","\n","  # ***************************COMPUTE DISCRIMINATION********************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","\n","  excelBook= load_workbook(r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_p3/LogReg/LR_Results.xlsx')\n","  Violent= excelBook['Violent']\n","  data= Violent.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n"," \n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF,\n","                                          'DATA_SP':dsp,'DATA_CONS':dif,'DATA_DI':ddi})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_p3/LogReg/LR_Results.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Violent', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CePqHL8vf83Y"},"source":["#LOGISTIC REGRESSION REPAIR LEVEL 0\n"]},{"cell_type":"code","metadata":{"id":"infNNuv-f83Y","executionInfo":{"status":"aborted","timestamp":1629030719281,"user_tz":-570,"elapsed":24,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":["for i in range(1,51,1):\n","\n","  train_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Train'\n","  train_path= os.path.join(train_url ,(\"Train\"+ str(i)+ \".csv\"))\n","  train= pd.read_csv(train_path)\n","  first_column = train.pop('two_year_recid')\n","  train.insert(0, 'two_year_recid', first_column)\n","\n","  test_url=r'/content/gdrive/MyDrive/Datasets/SurveyData/DATASET/Violent/Test'\n","  test_path= os.path.join(test_url ,(\"Test\"+ str(i)+ \".csv\"))\n","  test= pd.read_csv(test_path)\n","  first_column = test.pop('two_year_recid')\n","  test.insert(0, 'two_year_recid', first_column)\n","\n","   #********************************************************binary labels for DI Remover*************************************************************\n","\n","  bldTrain= BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=train,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  \n","  bldTest=  BinaryLabelDataset(favorable_label=1,\n","                                unfavorable_label=0,\n","                                df=test,\n","                                label_names=['two_year_recid'],\n","                                protected_attribute_names=['race'],\n","                                unprivileged_protected_attributes=[[0]],\n","                                privileged_protected_attributes=[[1]])\n","  #*******************************************************DI Remover instance**************************************************************\n","  DIR = DisparateImpactRemover(repair_level=0)\n","\n","  DI_Train = DIR.fit_transform(bldTrain )\n","  DI_Test = DIR.fit_transform(bldTest)\n","  \n","  #*****************************************Repaired Train and Test Set*******************************************************\n","  train=  pd.DataFrame(np.hstack([DI_Train .labels,DI_Train .features]),columns=train.columns)\n","  test=  pd.DataFrame(np.hstack([DI_Test .labels,DI_Test .features]),columns= test.columns)\n","  # TotalRepairedDF= pd.concat([RepairedTrain ,RepairedTest ])\n","\n","  # normalization of train and test sets\n","  Fitter= MM.fit(train)\n","  transformed_train=Fitter.transform(train)\n","  train=pd.DataFrame(transformed_train, columns= train.columns)\n","\n","  #test normalization\n","  transformed_test=Fitter.transform(test)\n","  test=pd.DataFrame(transformed_test, columns= test.columns)\n","\n","  # *************CHECKING FAIRNESS IN DATASET**************************\n","  ## ****************CONVERTING TO BLD FORMAT******************************\n","\n","\n","  #Transforming the Train and Test Set to BinaryLabel\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  # class Train(StandardDataset):\n","  #     def __init__(self,label_name= 'two_year_recid',\n","  #                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","  #         super(Train, self).__init__(df=train  , label_name=label_name ,\n","  #             favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","  #             privileged_classes=privileged_classes ,\n","  #            )\n","\n","\n","\n","\n","\n","  # BLD_Train= Train(protected_attribute_names= ['race'],\n","  #                        privileged_classes= [[1]])\n","\n","  class Test(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(Test, self).__init__(df=test  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_Test= Test(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","  ## ********************Checking Bias in Data********************************\n","  DataBias_Checker = BinaryLabelDatasetMetric(BLD_Test    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","\n","  dsp= DataBias_Checker .statistical_parity_difference() \n","  dif= DataBias_Checker.consistency()  \n","  ddi= DataBias_Checker.disparate_impact() \n","\n","  print('The Statistical Parity diference is = {diff}'.format(diff=  dsp  ))\n","  print('Individual Fairness is = {IF}'.format( IF=  dif ))\n","  print('Disparate Impact is = {IF}'.format( IF=   ddi  ))\n","  # ********************SETTING TO H20 FRAME AND MODEL TRAINING*******************************\n","  x = list(train.columns)\n","  y = \"two_year_recid\"\n","  x.remove(y)\n","  Train=h2o.H2OFrame(train)\n","  Test= h2o.H2OFrame(test)\n","  Train[y] = Train[y].asfactor()\n","  Test[y] = Test[y].asfactor()\n","  LogReg = H2OGeneralizedLinearEstimator(family= \"binomial\", lambda_ = 0)\n","  LogReg.train(x=x, y=y, training_frame=Train)\n","\n","  LogReg_Predictions= LogReg.predict(Test)\n","  LogReg_Predictions= LogReg_Predictions.as_data_frame()\n","  # *************************REPLACE LABELS OF DUPLICATED TEST SET WITH PREDICTIONS**************************************\n","  predicted_df= test.copy()\n","  predicted_df['two_year_recid']= LogReg_Predictions.predict.to_numpy()\n","\n","  # ***************************COMPUTE DISCRIMINATION********************************\n","\n","  advantagedGroup= [{'race':1}]\n","  disadvantagedGroup= [{'race':0}]\n","\n","  class PredTest(StandardDataset):\n","      def __init__(self,label_name= 'two_year_recid',\n","                  favorable_classes= [1],protected_attribute_names=['race'],   privileged_classes=[[1]], ):\n","\n","\n","          super(PredTest, self).__init__(df=predicted_df  , label_name=label_name ,\n","              favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n","              privileged_classes=privileged_classes ,\n","            )\n","\n","  BLD_PredTest= PredTest(protected_attribute_names= ['race'],\n","                        privileged_classes= [[1]])\n","\n","\n","  excelBook= load_workbook(r'/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_p0/LogReg/LR_Results.xlsx')\n","  Violent= excelBook['Violent']\n","  data= Violent.values\n","\n","  # Get columns\n","  columns = next(data)[0:]\n"," \n","  OldDF = pd.DataFrame(data, columns=columns)\n","\n","  ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n","  Accuracy= ClassifierBias.accuracy()\n","  TPR= ClassifierBias.true_positive_rate()\n","  TNR= ClassifierBias.true_negative_rate()\n","  NPV= ClassifierBias.negative_predictive_value()\n","  PPV= ClassifierBias.positive_predictive_value()\n","  SP=ClassifierBias .statistical_parity_difference() \n","  IF=ClassifierBias.consistency()\n","  DI=ClassifierBias.disparate_impact()\n","  EOP=ClassifierBias.true_positive_rate_difference()\n","  EO=ClassifierBias.average_odds_difference()\n","  FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n","  NPV_diff=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n","  FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n","  PPV_diff=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n","  BGE = ClassifierBias.between_group_generalized_entropy_index()\n","  WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n","  BGTI = ClassifierBias.between_group_theil_index()\n","  WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index()\n","  EDF= ClassifierBias.differential_fairness_bias_amplification()\n","\n","  newdf= pd.DataFrame(index = [0], data= { 'ACCURACY': Accuracy,'TPR': TPR, 'PPV':PPV, 'TNR':TNR,'NPV':NPV,'SP':SP,'CONSISTENCY':IF,'DI':DI,'EOP':EOP,'EO':EO,'FDR':FDR,'NPV_diff':NPV_diff, \n","                                          'FOR':FOR,'PPV_diff':PPV_diff,'BGEI':BGE,'WGEI':WGE,'BGTI':BGTI,'WGTI':WGTI,'EDF':EDF,\n","                                          'DATA_SP':dsp,'DATA_CONS':dif,'DATA_DI':ddi})\n","  newdf=pd.concat([OldDF,newdf])\n","\n","  pathway= r\"/content/gdrive/MyDrive/Datasets/SurveyData/RESULTS/DIR/level_p0/LogReg/LR_Results.xlsx\"\n","\n","  with pd.ExcelWriter(pathway, engine='openpyxl') as writer:\n","    #load workbook base as for writer\n","    writer.book= excelBook\n","    writer.sheets=dict((ws.title, ws) for ws in excelBook.worksheets)\n","    newdf.to_excel(writer, sheet_name='Violent', index=False)\n","    # newdf.to_excel(writer, sheet_name='Adult', index=False)\n","\n","  print('Accuracy', Accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YyfPmIRsIMuE","executionInfo":{"status":"aborted","timestamp":1629030719281,"user_tz":-570,"elapsed":23,"user":{"displayName":"DEHO OSCAR BLESSED","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhE0cXJBikzHY7xUavBkbPRseKZ_N-obrw0xIoLhQ=s64","userId":"04127040763952829247"}}},"source":[""],"execution_count":null,"outputs":[]}]}