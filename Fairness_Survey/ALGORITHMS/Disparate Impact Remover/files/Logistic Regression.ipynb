{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.metrics.classification_metric import ClassificationMetric\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.metrics import accuracy_score , classification_report, confusion_matrix, roc_auc_score,mean_squared_error,f1_score \n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "from aif360.datasets import StandardDataset , BinaryLabelDataset\n",
    "from sklearn.tree import  DecisionTreeClassifier \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from utils import make_dataset, display_results\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "MM= MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Unfair Model Using TrainTest split 1\n",
    "TrainData1= pd.read_csv(r\"C:\\Users\\dehob001\\OneDrive - University of South Australia(1)\\Python Projects\\Survey of Fairness \"\n",
    "                        r\"Algorithms\\Baselines\\German Credit DataSet\\5Fold German Splits\\Shuffle 1\\Train\\Train 5.csv\")\n",
    "TestData1= pd.read_csv(r\"C:\\Users\\dehob001\\OneDrive - University of South Australia(1)\\Python Projects\\Survey of Fairness \"\n",
    "                       r\"Algorithms\\Baselines\\German Credit DataSet\\5Fold German Splits\\Shuffle 1\\Test\\Test 5.csv\")\n",
    "\n",
    "#Trainset\n",
    "Train1= TrainData1.drop('default', axis= 1)\n",
    "TrainLabel1= TrainData1 ['default']\n",
    "Fitter1= MM.fit(Train1)\n",
    "Transformed_Train1= Fitter1.transform(Train1) \n",
    "Train1DF = pd.DataFrame(Transformed_Train1  , columns= Train1.columns)\n",
    "\n",
    "#TestSEt\n",
    "Test1= TestData1 .drop('default', axis= 1)\n",
    "TestLabel1= TestData1['default']\n",
    "Transformed_Test1= Fitter1.transform(Test1)\n",
    "Test1DF=pd.DataFrame(Transformed_Test1 , columns= Test1.columns)\n",
    "\n",
    "TrainTest= pd.concat([TrainData1, TestData1 ])\n",
    "\n",
    "# print(Train1DF.shape, Test1DF.shape, TrainLabel1.shape, TestLabel1.shape )\n",
    "# print(TrainData1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DI proportion calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #proportion calculator\n",
    "# # def calc_prop(data, group_col, group, output_col, output_val):\n",
    "# #     new = data[data[group_col] == group]\n",
    "# #     return len(new[new[output_col] == output_val])/len(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting  Train and Test into BinaryLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the Train and Test Set to BinaryLabel\n",
    "class Train(StandardDataset):\n",
    "    def __init__(self,label_name= 'default',\n",
    "                 favorable_classes= [1],protected_attribute_names=['age'],   privileged_classes=[[1]], ):\n",
    "\n",
    "\n",
    "        super(Train, self).__init__(df=TrainData1  , label_name=label_name ,\n",
    "            favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n",
    "            privileged_classes=privileged_classes ,\n",
    "           )\n",
    "\n",
    "\n",
    "\n",
    "advantagedGroup= [{'age':1}]\n",
    "disadvantagedGroup= [{'age':0}]\n",
    "\n",
    "BLD_Train= Train(protected_attribute_names= ['age'],\n",
    "                       privileged_classes= [[1]])\n",
    "\n",
    "\n",
    "\n",
    "class Test(StandardDataset):\n",
    "    def __init__(self,label_name= 'default',\n",
    "                 favorable_classes= [1],protected_attribute_names=['age'],   privileged_classes=[[1]], ):\n",
    "\n",
    "\n",
    "        super(Test, self).__init__(df=TestData1  , label_name=label_name ,\n",
    "            favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n",
    "            privileged_classes=privileged_classes ,\n",
    "           )\n",
    "\n",
    "\n",
    "\n",
    "advantagedGroup= [{'age':1}]\n",
    "disadvantagedGroup= [{'age':0}]\n",
    "\n",
    "BLD_Test= Test(protected_attribute_names= ['age'],\n",
    "                       privileged_classes= [[1]])\n",
    "\n",
    "# BLD_Train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking bias in Raw Data (Disparate Impact, Statistical Parity and consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Statistical Parity diference is = 0.07557268250909865\nIndividual Fairness is = [0.669]\nDisparate Impact is = 1.2563543936092954\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# You dont need to do this\n",
    "DataBias_Checker = BinaryLabelDatasetMetric(BLD_Test    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n",
    "\n",
    "dsp= DataBias_Checker .statistical_parity_difference() \n",
    "dif= DataBias_Checker.consistency()  \n",
    "ddi= DataBias_Checker.disparate_impact() \n",
    "\n",
    "print('The Statistical Parity diference is = {diff}'.format(diff=  dsp  ))\n",
    "print('Individual Fairness is = {IF}'.format( IF=  dif ))\n",
    "print('Disparate Impact is = {IF}'.format( IF=   ddi  ))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Accuracy and Predictive Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.755\n0.6811534379054134\n0.5504587155963302\n0.245\n"
     ]
    }
   ],
   "source": [
    "#Checking Prior Biases\n",
    "Clf= LogisticRegression( )\n",
    "Clf .fit(Train1DF, TrainLabel1)\n",
    "\n",
    "\n",
    "acc= accuracy_score( TestLabel1 , Clf .predict(Test1DF))\n",
    "roc= roc_auc_score(TestLabel1 , Clf.predict(Test1DF ))\n",
    "f1=f1_score(TestLabel1 ,Clf.predict(Test1DF))\n",
    "rmse= mean_squared_error(TestLabel1,Clf.predict(Test1DF), squared= True)\n",
    "\n",
    "print(acc)\n",
    "print( roc)\n",
    "print(f1)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Checking Discrimniation of Biased Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# predictedDF= pd.DataFrame({'Age': Test1DF['age'],'predictions': Predictions })\n",
    "# #print(predictedDF)\n",
    "\n",
    "# #Calculate DI of classifications\n",
    "# LR_upriv= calc_prop(predictedDF ,'Age',0,'predictions',1)\n",
    "# LR_priv= calc_prop(predictedDF ,'Age',1,'predictions',1)\n",
    "# print('Favourable outcome for unprivilege group:',LR_upriv )\n",
    "# print('Favourable outcome for privilege group: ',LR_priv )\n",
    "# print('DI: ', LR_upriv/LR_priv )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions = Clf.predict(Test1DF)\n",
    "PredictedDataSet= TestData1.copy(deep=True)\n",
    "PredictedDataSet['default'] = Predictions\n",
    "\n",
    "class PredTest(StandardDataset):\n",
    "    def __init__(self,label_name= 'default',\n",
    "                 favorable_classes= [1],protected_attribute_names=['age'],   privileged_classes=[[1]], ):\n",
    "\n",
    "\n",
    "        super(PredTest, self).__init__(df=PredictedDataSet  , label_name=label_name ,\n",
    "            favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n",
    "            privileged_classes=privileged_classes ,\n",
    "           )\n",
    "\n",
    "\n",
    "\n",
    "advantagedGroup= [{'age':1}]\n",
    "disadvantagedGroup= [{'age':0}]\n",
    "\n",
    "\n",
    "\n",
    "BLD_PredTest= PredTest(protected_attribute_names= ['age'],\n",
    "                       privileged_classes= [[1]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Statistical Parity diference is = 0.06508242346392634\nConsistency is = [0.669]\nDisparate Impact is = 1.2814814814814814\nEquality of Opportunity = 0.009803921568627472\nEqualized Odds = 0.03166184506589523\nPositive Predictive Value = 0.0\nFalse Discovery Rate Difference = 0.0\nNegative Predictive Value Difference = -0.06766917293233088\nFalse Omission Rate Difference = 0.06766917293233082\nBetween-Group Entropy Index= 7.349691058246566e-06\nWithin-Group Entropy Index = 0.13770033425758177\nBetween-Group Theil Index = 7.369857101799226e-06\nWithin-Group Theil Index= 0.20064147876767444\n"
     ]
    }
   ],
   "source": [
    "\n",
    "OldOdf= pd.read_csv(r\"C:\\Users\\dehob001\\OneDrive - University of South Australia(1)\\Python Projects\\Survey of Fairness Algorithms\\Disparate Impact Remover\\Odf.csv\")\n",
    "\n",
    "ClassifierBias = ClassificationMetric( BLD_Test,BLD_PredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n",
    "\n",
    "SP=ClassifierBias .statistical_parity_difference() \n",
    "IF=ClassifierBias.consistency()\n",
    "DI=ClassifierBias.disparate_impact()\n",
    "EOP=ClassifierBias.true_positive_rate_difference()\n",
    "EO=ClassifierBias.average_odds_difference()\n",
    "FDR= ClassifierBias.false_discovery_rate(privileged=False)- ClassifierBias.false_discovery_rate(privileged=True)\n",
    "NPV=ClassifierBias.negative_predictive_value(privileged=False)-ClassifierBias.negative_predictive_value(privileged=True)\n",
    "FOR=ClassifierBias.false_omission_rate(privileged=False)-ClassifierBias.false_omission_rate(privileged=True)\n",
    "PPV=ClassifierBias.positive_predictive_value(privileged=False) -ClassifierBias.positive_predictive_value(privileged=True)\n",
    "\n",
    "BGE = ClassifierBias.between_group_generalized_entropy_index()\n",
    "WGE = ClassifierBias.generalized_entropy_index()-ClassifierBias.between_group_generalized_entropy_index()\n",
    "BGTI = ClassifierBias.between_group_theil_index()\n",
    "WGTI = ClassifierBias.theil_index() -ClassifierBias.between_group_theil_index() \n",
    "\n",
    "print('The Statistical Parity diference is = {diff}'.format(diff=  SP ))\n",
    "print('Consistency is = {IF}'.format( IF= IF))\n",
    "print('Disparate Impact is = {IF}'.format( IF= DI))\n",
    "print('Equality of Opportunity = {IF}'.format( IF=EOP ))\n",
    "print('Equalized Odds = {IF}'.format( IF= EO))\n",
    "print('Positive Predictive Value = {IF}'.format( IF= PPV))\n",
    "print('False Discovery Rate Difference = {IF}'.format( IF= FDR ))\n",
    "print('Negative Predictive Value Difference = {IF}'.format( IF=NPV  ))\n",
    "print('False Omission Rate Difference = {IF}'.format( IF= FOR))\n",
    "print('Between-Group Entropy Index= {IF}'.format( IF= BGE ))\n",
    "print('Within-Group Entropy Index = {IF}'.format( IF= WGE ))\n",
    "print('Between-Group Theil Index = {IF}'.format( IF= BGTI))\n",
    "print('Within-Group Theil Index= {IF}'.format( IF= WGTI ))\n",
    "\n",
    "\n",
    "Odf= pd.DataFrame({'acc':acc,'roc':roc, 'f1':f1,'rmse':rmse,'SP':SP,'IF':IF,'DI':DI, 'EOP':EOP,'EO': EO, 'PPV':PPV,'FDR':FDR, 'NPV': NPV,'FOR':FOR,'BGE':BGE , 'WGE':WGE, 'BGTI':BGTI,'WGTI':WGTI})\n",
    "\n",
    "OldOdf= pd.concat([OldOdf, Odf ])\n",
    "OldOdf.to_csv('Odf.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repairing Biased Data Using Disparate Impact Remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               instance weights features                \\\n                                                         \n                                   month credit_amount   \ninstance names                                           \n0                           1.0     21.0        1216.0   \n1                           1.0     18.0        1366.0   \n2                           1.0     18.0        4272.0   \n3                           1.0     11.0         652.0   \n4                           1.0     11.0        7297.0   \n...                         ...      ...           ...   \n195                         1.0     11.0        1216.0   \n196                         1.0     27.0        2221.0   \n197                         1.0     11.0         601.0   \n198                         1.0     45.0        1845.0   \n199                         1.0     45.0        3016.0   \n\n                                                                     \\\n                                                                      \n               investment_as_income_percentage  sex residence_since   \ninstance names                                                        \n0                                          4.0  1.0             4.0   \n1                                          3.0  0.0             4.0   \n2                                          1.0  0.0             4.0   \n3                                          4.0  1.0             4.0   \n4                                          1.0  0.0             2.0   \n...                                        ...  ...             ...   \n195                                        3.0  0.0             4.0   \n196                                        4.0  1.0             4.0   \n197                                        4.0  1.0             4.0   \n198                                        4.0  1.0             4.0   \n199                                        3.0  1.0             4.0   \n\n                                                                        \\\n               protected attribute                                       \n                               age number_of_credits people_liable_for   \ninstance names                                                           \n0                              1.0               1.0               2.0   \n1                              1.0               1.0               1.0   \n2                              0.0               2.0               1.0   \n3                              1.0               1.0               1.0   \n4                              0.0               1.0               1.0   \n...                            ...               ...               ...   \n195                            1.0               1.0               1.0   \n196                            1.0               1.0               1.0   \n197                            1.0               1.0               1.0   \n198                            0.0               1.0               1.0   \n199                            1.0               1.0               1.0   \n\n                        ...                                                 \\\n                        ...                                                  \n               sex-age  ... housing_A153 skill_level_A171 skill_level_A172   \ninstance names          ...                                                  \n0                  0.0  ...          1.0              0.0              0.0   \n1                  0.0  ...          0.0              0.0              1.0   \n2                  0.0  ...          0.0              0.0              0.0   \n3                  0.0  ...          0.0              0.0              0.0   \n4                  0.0  ...          0.0              1.0              0.0   \n...                ...  ...          ...              ...              ...   \n195                0.0  ...          0.0              0.0              1.0   \n196                0.0  ...          0.0              0.0              0.0   \n197                0.0  ...          0.0              0.0              0.0   \n198                0.0  ...          1.0              0.0              0.0   \n199                0.0  ...          0.0              0.0              0.0   \n\n                                                                 \\\n                                                                  \n               skill_level_A173 skill_level_A174 telephone_A191   \ninstance names                                                    \n0                           1.0              0.0            1.0   \n1                           0.0              0.0            0.0   \n2                           1.0              0.0            1.0   \n3                           1.0              0.0            1.0   \n4                           0.0              0.0            1.0   \n...                         ...              ...            ...   \n195                         0.0              0.0            1.0   \n196                         0.0              1.0            0.0   \n197                         1.0              0.0            1.0   \n198                         1.0              0.0            0.0   \n199                         1.0              0.0            1.0   \n\n                                                                      labels  \n                                                                              \n               telephone_A192 foreign_worker_A201 foreign_worker_A202         \ninstance names                                                                \n0                         0.0                 1.0                 0.0    0.0  \n1                         1.0                 1.0                 0.0    0.0  \n2                         0.0                 1.0                 0.0    0.0  \n3                         0.0                 1.0                 0.0    0.0  \n4                         0.0                 1.0                 0.0    0.0  \n...                       ...                 ...                 ...    ...  \n195                       0.0                 1.0                 0.0    0.0  \n196                       1.0                 1.0                 0.0    0.0  \n197                       0.0                 1.0                 0.0    0.0  \n198                       1.0                 1.0                 0.0    1.0  \n199                       0.0                 1.0                 0.0    0.0  \n\n[200 rows x 61 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR = DisparateImpactRemover(repair_level=1)\n",
    "\n",
    "DI_Train = DIR.fit_transform(BLD_Train )\n",
    "DI_Test = DIR.fit_transform(BLD_Test)\n",
    "DI_Test \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repaired Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "RepairedTrain=  pd.DataFrame(np.hstack([DI_Train .labels,DI_Train .features]),columns=TrainData1.columns)\n",
    "RepairedTest=  pd.DataFrame(np.hstack([DI_Test .labels,DI_Test .features]),columns=TestData1.columns)\n",
    "TotalRepairedDF= pd.concat([RepairedTrain ,RepairedTest ])\n",
    "#TotalRepairedDF.to_csv('repaired.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking bias in Repaired Data (Disparate Impact, Statistical Parity and consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact is = 1.2563543936092954\nThe Statistical Parity diference is = 0.07557268250909865\nIndividual Fairness is = [0.673]\n"
     ]
    }
   ],
   "source": [
    "Olddb= pd.read_csv(r\"C:\\Users\\dehob001\\OneDrive - University of South Australia(1)\\Python Projects\\Survey of Fairness Algorithms\\Disparate Impact Remover\\db.csv\")\n",
    "\n",
    "FairDataBias_Checker = BinaryLabelDatasetMetric(DI_Test    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n",
    "fdi= FairDataBias_Checker.disparate_impact()  \n",
    "fsp=FairDataBias_Checker .statistical_parity_difference()\n",
    "fif= FairDataBias_Checker.consistency() \n",
    "\n",
    "print('Disparate Impact is = {IF}'.format( IF=  fdi  ))\n",
    "print('The Statistical Parity diference is = {diff}'.format(diff= fsp   ))\n",
    "print('Individual Fairness is = {IF}'.format( IF= fif  ))\n",
    "\n",
    "DataBias= pd.DataFrame({'DI':ddi, 'SP':dsp, 'IF':dif,'FDI':fdi,'FSP':fsp, 'FIF':fif})\n",
    "Olddb= pd.concat([Olddb, DataBias ])\n",
    "Olddb .to_csv('db.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up  Repaired TrainTest for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer= RepairedTrain .drop('default', axis= 1)\n",
    "TrainLabel= RepairedTrain ['default']\n",
    "Fitter= MM.fit(Trainer)\n",
    "Transformed_Train= Fitter.transform(Trainer) \n",
    "TrainDF = pd.DataFrame(Transformed_Train , columns= Trainer.columns)\n",
    "\n",
    "#TestSEt\n",
    "Tester= RepairedTest.drop('default', axis= 1)\n",
    "TestLabel= RepairedTest ['default']\n",
    "Transformed_Test= Fitter.transform(Tester)\n",
    "TestDF=pd.DataFrame(Transformed_Test , columns= Tester.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression  Repaired Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.765\n0.6837480835004128\n0.5523809523809523\n0.235\n"
     ]
    }
   ],
   "source": [
    "#Checking Fair metrics\n",
    "Clfr= LogisticRegression( )\n",
    "Clfr.fit(TrainDF, TrainLabel)\n",
    "\n",
    "acc= accuracy_score( TestLabel , Clfr .predict(TestDF))\n",
    "roc= roc_auc_score(TestLabel , Clfr.predict(TestDF ))\n",
    "f1=f1_score(TestLabel ,Clfr.predict(TestDF))\n",
    "rmse= mean_squared_error(TestLabel,Clfr.predict(TestDF), squared= True)\n",
    "\n",
    "print(acc)\n",
    "print( roc)\n",
    "print(f1)\n",
    "print(rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Fair Predictions to BLD for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predicton = Clfr.predict(TestDF)\n",
    "FairPredictedData= TestData1.copy(deep=True)\n",
    "FairPredictedData['default'] = Predicton\n",
    "\n",
    "class FairPredTest(StandardDataset):\n",
    "    def __init__(self,label_name= 'default',\n",
    "                 favorable_classes= [1],protected_attribute_names=['age'],   privileged_classes=[[1]], ):\n",
    "\n",
    "\n",
    "        super(FairPredTest, self).__init__(df=FairPredictedData  , label_name=label_name ,\n",
    "            favorable_classes=favorable_classes , protected_attribute_names=protected_attribute_names ,\n",
    "            privileged_classes=privileged_classes ,\n",
    "           )\n",
    "\n",
    "\n",
    "\n",
    "advantagedGroup= [{'age':1}]\n",
    "disadvantagedGroup= [{'age':0}]\n",
    "\n",
    "\n",
    "\n",
    "BLD_FairPredTest= FairPredTest(protected_attribute_names= ['age'],\n",
    "                       privileged_classes= [[1]])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking For Bias in Fair Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Statistical Parity diference is = 0.08820381074716332\nConsistency is = [0.669]\nDisparate Impact is = 1.4238683127572016\nEquality of Opportunity = 0.02941176470588236\nEqualized Odds = 0.053760848601735786\nPositive Predictive Value = -0.04166666666666663\nFalse Discovery Rate Difference = 0.041666666666666685\nNegative Predictive Value Difference = -0.06607760276603925\nFalse Omission Rate Difference = 0.06607760276603916\nBetween-Group Entropy Index= 1.1126587327766335e-05\nWithin-Group Entropy Index = 0.136018614521693\nBetween-Group Theil Index = 1.1089439995285496e-05\nWithin-Group Theil Index= 0.20245080960431627\n"
     ]
    }
   ],
   "source": [
    "Olddf= pd.read_csv(r\"C:\\Users\\dehob001\\OneDrive - University of South Australia(1)\\Python Projects\\Survey \"\n",
    "                   r\"of Fairness Algorithms\\Disparate Impact Remover\\df.csv\")\n",
    "FairClassifierBias = ClassificationMetric( BLD_Test,BLD_FairPredTest    , unprivileged_groups= disadvantagedGroup, privileged_groups= advantagedGroup)\n",
    "SP=FairClassifierBias .statistical_parity_difference() \n",
    "IF=FairClassifierBias.consistency()\n",
    "DI=FairClassifierBias.disparate_impact()\n",
    "EOP=FairClassifierBias.true_positive_rate_difference()\n",
    "EO=FairClassifierBias.average_odds_difference()\n",
    "FDR= FairClassifierBias.false_discovery_rate(privileged=False)- FairClassifierBias.false_discovery_rate(privileged=True)\n",
    "NPV=FairClassifierBias.negative_predictive_value(privileged=False)-FairClassifierBias.negative_predictive_value(privileged=True)\n",
    "FOR=FairClassifierBias.false_omission_rate(privileged=False)-FairClassifierBias.false_omission_rate(privileged=True)\n",
    "PPV=FairClassifierBias.positive_predictive_value(privileged=False) -FairClassifierBias.positive_predictive_value(privileged=True)\n",
    "\n",
    "BGE = FairClassifierBias.between_group_generalized_entropy_index()\n",
    "WGE = FairClassifierBias.generalized_entropy_index()-FairClassifierBias.between_group_generalized_entropy_index()\n",
    "BGTI = FairClassifierBias.between_group_theil_index()\n",
    "WGTI = FairClassifierBias.theil_index() -FairClassifierBias.between_group_theil_index() \n",
    "\n",
    "print('The Statistical Parity diference is = {diff}'.format(diff=  SP ))\n",
    "print('Consistency is = {IF}'.format( IF= IF))\n",
    "print('Disparate Impact is = {IF}'.format( IF= DI))\n",
    "print('Equality of Opportunity = {IF}'.format( IF=EOP ))\n",
    "print('Equalized Odds = {IF}'.format( IF= EO))\n",
    "print('Positive Predictive Value = {IF}'.format( IF= PPV))\n",
    "print('False Discovery Rate Difference = {IF}'.format( IF= FDR ))\n",
    "print('Negative Predictive Value Difference = {IF}'.format( IF=NPV  ))\n",
    "print('False Omission Rate Difference = {IF}'.format( IF= FOR))\n",
    "print('Between-Group Entropy Index= {IF}'.format( IF= BGE ))\n",
    "print('Within-Group Entropy Index = {IF}'.format( IF= WGE ))\n",
    "print('Between-Group Theil Index = {IF}'.format( IF= BGTI))\n",
    "print('Within-Group Theil Index= {IF}'.format( IF= WGTI ))\n",
    "df=pd.DataFrame({'acc':acc,'roc':roc, 'f1':f1,'rmse':rmse,'SP':SP,'IF':IF,'DI':DI, 'EOP':EOP,'EO': EO, 'PPV':PPV,'FDR':FDR, 'NPV': NPV,'FOR':FOR,'BGE':BGE , 'WGE':WGE, 'BGTI':BGTI,'WGTI':WGTI})\n",
    "\n",
    "Olddf= pd.concat([Olddf,df])\n",
    "Olddf.to_csv('df.csv') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "neptune": {
   "notebookId": "fc6bf6a6-d8ea-4418-93a9-d385209902bd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
